A ``primitive oriented'' scripting language core.

Libprim is a collection of primitive objects and operations written in
C, mostly intended for writing embedded audio/video applications.

In addition it contains an approach to assemble these objects as
primitives of a dynamic scripting core (VM).  This is an exploratory
project, roughly based on the following goals and priciples:

   * Create an ``invisible'' scripting language that integrates well
     with a C/C++ based project, and can be ``optimized out'' when
     necessary, yielding a static C/C++ app.

   * Focus on two memory/control models for the scripting language:
     Scheme and Packet Forth - a language based on a linear, dual
     stack memory model.

   * Look at C primitives as lightweight tasks.  Along side other
     benefits of concurrency-oriented programming, the property of
     interest is the ability to decouple the C side from the scripting
     language's _memory_ and _control_ models.

   * Concurrency-oriented programming allows intermediate composite
     data types to be replaced with serial channels and protocols
     (replace data aggregation with control flow structures).

   * Simple memory model: C primitives use only atoms which are either
     C primitive values (integers, floats, structure types), or
     abstract objects (pointers) with explicit ownership.




Entry: A bit less cryptic
Date: Tue Aug  4 11:14:36 CEST 2009

From a practical day-to-day problem solving pov, C is not going
anywhere because next to a programming language, it is also an
interface standard.  Anything that tries to incorporate modern
language techniques in an embedded programming project needs to play
nice with C.

So, instead of focussing on building new languages, I think it is
worthwhile to do some semantics engineering on subsets of C itself, by
providing an infrastructure that makes it easier to bootstrap
``functional, concurrency oriented C code''.

Hopefully this idea will become easier to articulate once the
implementation is done.

The goal is this:

   * Write a re-usable collection of C primitives necessary to support
     a dynamically typed scripting language, but do this in such a way
     that it does _not_ depend on external memory or control models.

   * To test the primitives and the programming method, write a couple
     of dyntyped scripting languages (memory and control models) on
     top of the primitives.  I'm thinking about two: a PF-like
     concatenative linear stack language and a Scheme-like
     CONS/CONT/GC based language.

   * Write a compiler for static memory allocation and scheduling to
     optimize away the scripting layer.

Static types?  Given that the goal of the project is _prototyping_
which is mainly _evolving specification_ I think it's better to stick
to a simpler dynamically typed approach.


Entry: CEKS machine
Date: Tue Aug  4 11:25:38 CEST 2009

Scheme implemented in terms of a CEKS[1] machine.  Focus on the
evaluator first and ignore GC.  Some CEKS machine for
Javascript[3][4].

It might be easier to start with the CEK machine, and add the store
and GC later.

Anyways..  I started with the GC.  Bottom up seems better.  It already
gives a way to encode types.

[1] http://www.cs.utah.edu/plt/publications/pllc.pdf
[2] md5://3ab517a1167f86ef52caf3bf1a4fdb0c
[3] http://wiki.ecmascript.org/doku.php?id=meetings:dave_herman_presentation
[4] http://www.ccs.neu.edu/home/dherman/javascript/
[5] http://www.cs.utah.edu/~mflatt/past-courses/cs6520/public_html/s00/secd.ps

Entry: Lightweight tasks
Date: Tue Aug  4 12:20:48 CEST 2009

Based on the interface present in GNU PTH[1].

[1] http://www.gnu.org/software/pth/



Entry: Primitives and Composition, an OS?
Date: Tue Aug  4 12:25:20 CEST 2009

Any form of language design is based on building primitives (axioms)
and composition mechanism (inference rules).

Is is a stretch to say that an Operating System is actually a
composition mechanism?  If tasks are primitives, gluing tasks together
is what the OS does.


Entry: Aspect oriented programming
Date: Tue Aug  4 12:29:47 CEST 2009

The idea I'm trying to encode is the addition of join points to a C
program: make all these points available to an observer (scripting
language).

[1] http://en.wikipedia.org/wiki/Aspect_oriented_programming


Entry: Garbage Collector
Date: Tue Aug  4 14:53:52 CEST 2009

I've added a simple copying GC in ceks/gc.c for allocating vectors, as
a memory model for implementing the Scheme interpreter.  It uses
tagged objects: vectors, atoms, integers, and one available user tag.

The vectors themselves can be tagged (i.e. to implement a couple of
primitive structs) when the maximum vector size size is limited.

[1] http://www.brpreiss.com/books/opus5/html/page426.html

Entry: Symbols
Date: Tue Aug  4 17:58:47 CEST 2009

Instead of using a hash table, a preliminary implementation could just
use a stack.  This will make parsing slower, but can be easily changed
later.


Entry: Continuations
Date: Tue Aug  4 20:32:34 CEST 2009

So.. ISWIM uses n-ary primitive application, but unary abstraction and
application.  Why is this?  It seems that it makes the extension of
the environment on application easier: only a single variable has to
be added.

Maybe this isn't necessary for Scheme.  A multivalued application adds
all variables at the same time.  Instead of arg,fun,opd as explained
in PLLC, I needs just a single type that evaluates all variables in an
application from left to right, starting with the function position,
and then performs either an application (extending the environment) or
a primitive evaluation (production of a value).

So how to represent (x x x _ x x x) where "_" is the hole?

In short: what does the CEKS machine do?

   It converts the current expression and its continuation in a
   simplified expression and an updated continuation.  It is
   _application_ (triggered by the availability of the last argument)
   that _pops_ the K stack.

So.. Representing continuations.

(((f a b) E) K) -> ( _ (((f E) (a E) (b E)) K))
                -> ((f E) (((a E) (b E)) K))



Entry: Objects and Lambdas
Date: Wed Aug  5 09:43:14 CEST 2009

What I'm trying to accomplish probably has a lot of parallells with
COLA[1].  However, my concern is mostly interoperability with C.

Anyways.  GC and objects.  It might be wise to require objects to
exhibit a class structure: each object points to a record of methods,
where the first couple are used by the GC.
 

[1] http://piumarta.com/software/cola/


Entry: Confused about primitives
Date: Wed Aug  5 13:14:07 CEST 2009

This is ironic: the goal is to properly define primitives, but I'm
already getting hosed!

Once conclusion to draw: there is a set of C primitives that is best
written directly in the required API (sc ,object, ...) -> object
instead of in unwrapped C, because of the tight coupling with the data
representation.  These _direct primitives_ are written manually.

In short: to keep things simple, the interpreter is written in terms
of primitive functions using all scheme datatypes (ala tinyscheme).

What cannot be avoided however is functions that convert between
scheme and C values (esp. strings<-> and ints).  it's best to limit
the use however, and try to use as much as possible real scheme
primitives to avoid duplication (don't give in to premature opti
here!).

So, let's try it:

          All C functions used in the implementation of the Scheme
          interpreter are Scheme primitives.

This fixes the primitive calling convention to:

object fn(sc*, object, ...)

This will raise other problems since I'm not sure it's possible to
have variable argument invokation (assumed from Pure Data source).


Entry: Representing constants
Date: Wed Aug  5 13:54:45 CEST 2009

I might be useful to make it possible to directly cast non-managed
objects, by assuming the class pointer is in the slot _before_ the
pointer.  It is upto the user then to _never_ place them inside GC
managed vectors, as they won't have a valid class pointer.

Elaborate on this a bit..

I.e. it would be nice for a (const char *) to be a valid object that
can reside in a collected data structure.  I.e. the object itself
should be recognizable as not requiring free().

The problem then is that I'm running out of tags:

00 non-managed pointer
01 vector
10 integer
11 bool

Using more than 2 tag bits places extra contstraints on pointer
alignment.  Booleans are useful for predicate implementation in
Scheme.

It is possible however to use the 01 vector slot, since we know what
that will point to:
     - integer (it's a live vector, and the int = size)
     - vector  (moved object)

This could be a non-managed pointer to represent a class?

Maybe another workaround is simpler: doubly wrapping all predicates
and representing bools as integers.

Let's stick to what we have and thing about this a bit more.. It looks
like bools are more useful than constants.  

Otoh: since bools require only two values, and are actually constants,
the problem is solved:


00 constant = non-managed pointer
01 vector
10 integer
11 managed pointer


Let's implement this first before rewriting everything to primitives.


Entry: Unsafe pointers
Date: Wed Aug  5 15:49:26 CEST 2009

I get it now: by making sure the atom_class pointer is the slot one
_before_ the atom pointer, it is possible to write unsafe functions
without too much hassle.

The free() pointer is stored in a class object.  This class object is
the value's type.  Some values however are only statically typed, and
appear as unsafe pointers if they make it into Scheme land.



Entry: debugging
Date: Wed Aug  5 17:48:59 CEST 2009

Hmm.. it's more difficult than I thought.  The interpreter itself I
can probably get debugged, since its structure is quite
straightforward.  However, the GC is going to be an adventure.
Currently I have no way to properly mark roots in case GC is
triggered inside a C primitive.

During the evaluation of step() we could push all allocated atoms to a
stack, and discard it at the end.  This makes sure that intermediate
data will not be freed.

Alternatively, the heap could be marked such that all objects past the
marker need to be copied first.

But this then wouldn't work if there are 2 or more collections.

Alternatively, the marking point could be translated into a vector
that is then added to a local stack of references.

The real question is: does the interpreter know what to mark?

Let's change a gc->root into a callback function.

Done.

Now it shouldn't be to hard to turn a gc marker into an array.
Basicly something like this:

        sc->marker = gc_marker(sc->gc)
        step()
        free_retain_stack()

If there is a collection we do something like

        push_retain_stack(gc_marker_to_vector(sc->marker))
        mark_all()
        sc->marker = gc_marker(sc->gc)


OR

do it manually.
which is probably too error-prone..

Ok.. what about keeping most of it hidden inside the GC.

When the GC calls mark_roots() it will pass in a vector of references
containing the objects that were saved since the last border mark.

Actually, this fancy trick doesn't work because the C stack still has
the _old_ pointers.

Not simple!

One option would be to abort the primitive whenever a collection is
necessary.  If they are written in a purely functional style this
shouldn't be a problem.


Entry: mixing GC and C
Date: Wed Aug  5 19:44:17 CEST 2009

The problem is this: a gc_alloc() inside a primitive might trigger a
collection.  At that point, all the pointers on the C stack will
become invalid.

To overcome this, all primitives that trigger gc_alloc() should be
purely functional, such that an allocation can restart the interpreter
step() that triggered the gc_alloc().

So.. If the gc itself is also made stateless (and written in CPS) then
this could work pretty well.

Lets see if gc_alloc() can be aborted.

Looks like it.. One problem however is that the current implementation
of gc_grow() will never be reached, so the program will end up in an
infinite loop when there isn't enough memory to account for a single
primitive execution.  Here the mark_wild() function would come in
handy:

If it is detected that a single primitive step cannot continue because
there is not enough heap space, the heap needs to grow.  As long as
gc_collect() doesn't abort the current GC invocation, this can be done
automatically.


Entry: constant strings
Date: Wed Aug  5 21:29:40 CEST 2009

Since these are not aligned, they cannot be objects.  So, no
constant strings.

Some goes for primitives.


Entry: First interpreter run
Date: Thu Aug  6 02:15:41 CEST 2009

# (zero? 123)

tom@zni:~/libprim/ceks$ ./gc-test
#state(#closure((zero? 123) ()) ())
#state(#closure(zero? ()) #frame(() (#closure(123 ())) ()))
#state(#closure(#prim<0x4014f3:1> ()) #frame(() (#closure(123 ())) ()))
#state(#closure(123 ()) #frame((#closure(#prim<0x4014f3:1> ())) () ()))
#state(#closure(#f ()) ())
ERROR: halt: #state(#closure(#f ()) ())
Trace/breakpoint trap



So primitive execution seems to work.  Still to test: abstraction and
application.  After fixing some bugs we're at:

# ((lambda (abc) 123) 456)

tom@zni:~/libprim/ceks$ ./gc-test
#state(#closure(((lambda (abc) 123) 456) ()) ())
#state(#closure((lambda (abc) 123) ()) #frame(() (#closure(456 ())) ()))
#state(#closure(#lambda(#(abc) 123) ()) #frame(() (#closure(456 ())) ()))
#state(#closure(456 ()) #frame((#closure(#lambda(#(abc) 123) ())) () ()))
#state(#closure(123 ((abc . #closure(456 ())))) ())
ERROR: halt: #state(#closure(123 ((abc . #closure(456 ())))) ())
Trace/breakpoint trap


Entry: Pre-emption
Date: Thu Aug  6 10:46:21 CEST 2009

So, let's install the preemption based garbage collector + write the
first `functional C' primitive task for parsing s-expressions = a
token enumerator.


Entry: constants vs. closures
Date: Thu Aug  6 12:09:08 CEST 2009

Yesterday went well.  The basic infrastructure seems to work.  Now I
need to get the small errors in the representation right.

Currently I have constants tagged to an environment.  This won't hurt,
but doesn't make much sense.  Maybe change the rep a bit.

The environment contains closures (open_term + environment) or
constants.

OK: it's way simpler to make everything a closure, but tag contstants
with an empty environment.

So.. Something is not right.  This gives an error:

   (post (cons 111 ((lambda (abc) 123) 456)))

ERROR: undefined: abc

I currently lost oversight so let's re-connect with the textbook
rules.


Entry: CEK primitives
Date: Thu Aug  6 15:01:19 CEST 2009

Looks like I'm confusing two things:

    * primitive values + fully reduced closures  (lambda + env)

    * non-fully reduced closures (application/varref/value + env)

I'd like to make sure that primitives only see primitive values and
fully reduced closures.  Also, primitives should be able to return
closures (even not fully reduced).

Where is the inconsistency?

Looking at PLLC[1] Chapter 6, p 74 at the CEK reduction rule [cek5], it
seems that primitives get passed _only_ primitive values, not
closures.

How can I make this consistent with a primitive that can operate on
closures as values?

Hmm.. Let's provide better names for the data structure.

1. A continuation frame represents the current state of argument
   evaluation from reducable closures to irreducable closures.

2. A fully evaluated continuation frame can be _applied_ which means
   that a new (reducable) closure is created by extending the closure
   in the first position with variables.


From what I could test it seems that application works fine.  So I
guess I need to figure out how to understand the difference between:

      - a machine where _all_ values are closures, but primitives can
        only accept and return primitive values (which have their
        environment dropped before primitive evaluation)

      - a machine where values are either closures or primitive
        values, and primitive functions can operate on both.

So, either the interpreter's notion of "closure" is made abstract, or
it is changed to directly operate on both primitive values and
closures.

What I could do is the following: regard interpreter state and
environment + data structures as different worlds.

STATE: the current reducable closure and continuation contain closure
structs only, but all communucation with the environment (extension
and reference) and primtives will pack/unpack closures to variables.

Ok. Added closure_pack() and closure_unpack().

[1] http://www.cs.utah.edu/plt/publications/pllc.pdf
[2] md5://3ab517a1167f86ef52caf3bf1a4fdb0c


Entry: Lists vs. applications
Date: Thu Aug  6 16:44:38 CEST 2009

Currently the implementation doesn't distinguish between applications
and lists.  The CEK machine doesn't deal with `constructors' which are
essentially non-evaluated functions.

I don't really know how this is done in practice.  Let's just wrap
code in a syntax object for now.

This seems to work.  There are now two kinds of terms: non-reduced
terms are SYNTAX while reduced terms are anything else.

Anyways.. It works, but it's probably a bit unorthodox.  This can't
represent syntax as values.  The real solution would be to represent
special forms, applications, variables references and quotes as
separate entities.

This will have to do for now.


Entry: Testing gc preemption
Date: Thu Aug  6 17:47:06 CEST 2009

Seems to work.  There is a problem though: triggering GC outside of
the mainloop is not well-defined.

This basicly boils down to the fact that the GC managed memory is
owned by the interpreter.  The caller of _sc_run() has no business
using these data structures.

It's useful for testing though.  It's necessary to perform a GC before
doing any allocation to prevent corruption.



Entry: Working?
Date: Thu Aug  6 18:51:02 CEST 2009

Looks like it's working. 

tom@zni:~/libprim/scheme$ wc -l *.[ch]
  167 gc.c
   10 gc_config.h
  133 gc.h
   54 main.c
  559 scheme.c
  201 scheme.h
   38 scheme_prim.h
   41 symbol.c
   28 symbol.h
 1231 total

It has almost nothing though.  Only evaluation + supporting prims.
Missing:
        - special forms (if, set!)
        - macros
        - reader
        - ports

The special forms might be enough so reader can maybe be written in
Scheme and compiled to C spec of the syntax tree?

Both special forms need modification to the continuation.  Assignment
will need to be done in such a way that GC interrupt isn't possible.

I'm adding a couple of structs to accomodate these different
continuation types.

Once realizing the frame type is only one of different kinds of
continuations, it was quite straigtforward to add 'if', 'set!',
'begin' and 'lambda' with multiple expressions.


Entry: Coroutines
Date: Thu Aug  6 19:30:26 CEST 2009

This paper defends the revival of (asymmetric) coroutines as a general
control abstraction[1].

[1] http://lambda-the-ultimate.org/node/2868


Entry: Macros
Date: Fri Aug  7 09:17:45 CEST 2009

This will require a syntax-level environment, and a lookup for every
form.

A macro continuation is simply a tag: the current value (s-expression)
needs to be tagged such that it is wrapped as SYNTAX and passed to the
interpreter.

I'm going to change the wording.  What is now SYNTAX should become
AST: a tag for reducable term.  SYNTAX in Scheme sense seems to be
something different: it is a data structure with binding information.
Even if I'm not going to support the latter, it's best not to confuse
things.

Ok, macros work by:  constructing a k_apply linked to k_macro.  The
k_apply is filled so it will trigger the application of the macro.
The k_macro wraps the result as an AST to trigger reduction.

Entry: C-Scheme
Date: Fri Aug  7 10:23:06 CEST 2009

Working with C-callable Scheme primitives makes the step towards a
PreScheme-like implementation easier.  Using only `let*', `if',
`begin', `set!', and a restriction on function application (only
primitives can be called from primitives) this could be turned into a
simple Scheme->C compiler.

At least, the way the interpreter step is implemented now in C +
Scheme datatypes, could be embedded into equivalent Scheme quite
easily.

        Scheme          C
        --------------------------------------------
        let*            Lexical variables and blocks.
        begin           Blocks.
        if              if statement.
        set!            Variable assignment.
        (fn arg ...)    Function call.
        prim-fn         C-implemented prim (or compiled from C-Scheme)
        
The only thing that won't work is `lambda'.  But, in case it is used
to pass to higher order functions (HOF), the HOF could be replaced
with a macro.

Note that PreScheme[1] is much more than this: it compiles a
significantly larger subset of Scheme.

Use pattern matching.


[1] http://en.wikipedia.org/wiki/PreScheme
[2] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.3.4031


Entry: Really done?
Date: Fri Aug  7 11:38:25 CEST 2009

It looks like I have all of Scheme now.  Next: make some C-code
analysis tools that generate all the boilerplate.

One more thing is missing: variable length argument lists.  This is
necessary to implement `list' without which macros are no fun.


Entry: Two Kinds of Primitives
Date: Fri Aug  7 13:20:51 CEST 2009

Due to restart at GC, the interpreter can support two kinds of
primitives:

     * RESTARTABLE: Pure functions operating on Scheme data structures
       (or impure functions that do not perform allocation _after_
       mutation).

     * ABSTRACT: C code that does not refer to any Scheme data.

The disadvantage of not being able to access Scheme data from impure C
code can be largely removed by providing a suspension mechanism for
primitives.  In this case the C code could behave as a coroutine,
which allows the use of enumerators / iterators instead of
construction of intermediate data.

Impure functions performing allocation before mutation are hard to
write, and are best limited to the implementation of internal
primitives supporting mutation (like `set!' and `define') and adaptor
code that bridge Scheme and C datastructures for abstract functions.

In short: use either purely functional programming or concurrency
oriented programming, and you get a very simple interpreter and
application structure.


Entry: Bootstrapping
Date: Fri Aug  7 19:14:57 CEST 2009

Bootstrapping is fun.  It's the first time I did this for a Scheme
interpreter.  I've added two mzscheme .ss files.  One to convert .scm
files to .c_ files as long as there is no reader, and one to lift the
primitives from the .c file and write boilerplate registration code.

Then bootstrapping the rest of the basic scheme functionality on top
of the bare words is quite interesting.

One thing though: I don't immediately see how to make apply and eval
work.  Eval is probably iterating the step() function.  But apply?
Similarly maybe?  Constructing a continuation frame and iterating the
interpreter?

There is probably a simpler though non-obvious way to do this..
Manipulating the machine state from inside a primitive would probably
do the trick.

The trick is that the function modifies the continuation.  This could
be done using mutation, but that won't play nice with running the
interpreter manually (as part of debugging for instance).  It's
probably best to make it possible for primitives to return a
continuation.  Or better: to allow for explicit "state transforming"
primitives.

Let's thinkg about this first..

Before the step() function can really be a pure function, it has to
isolate all side effects, and should not rely on a main loop's
actions.

Maybe it's best to first try to implement call/cc.

Applying a continuation is really simple.  But how to capture it?
Looks like that's similar to apply.

Then about errors.  Maybe it is better to keep errors local to the
step() function that generated them, and return them as values
containing the continuation.  Except for GC restarts: they need to go
all the way up to make sure data is not corrupted.

Ok. errors are now local to step() allowing for recursive step() but
GC restart goes all the way up the chain.

Then a surprise: eval is (make-ast '(post 'foo-foo))

If a primitive returns a reducable term (represented by the ast
wrapper type), it will be evaluated.

Maybe apply can be made in a similar way?  Well.. I found something
that uses eval and map and quote.  Not pretty..

Looks like manipulating the continuation is best.


Entry: letcc -> apply
Date: Sat Aug  8 10:22:47 CEST 2009

It seems better to stick with the way things work right now, and use
only a single type of primitive.  This makes integration with C (the
eventual goal) a lot easier, and will prevent a lot of if-type style
code.

With `letcc' implemented as a special form, it is now possible to
create primitives that operate on continuations as data, creating new
continuations.  These values can themselves be applied.  This means
`apply' can be implemented as a continuation modification followed by
an application.

Yes, but.  The current k_apply type is a bit awkward, because it
evaluates from left to right, it will wait for the final argument
before coninuing with application.  Maybe it's simplest to support a
right-to-left app frame?

Before continuing, let's make the continuations somewhat abstract:
each with a parent slot in the first position.

Ok.

One way is to have a reverse k_apply: one that traverses the arguments
from right to left.  This way the `apply' continuation can be
constructed as an argument list, with the function value passed to it.
This would either require an extra reverse, or some re-arrangement in
the primitive call trampoline.

Or, a different kind of continuation could be made that ignores its
value, but passes closure to its parent continuation.

Basicly, such a continuation takes a value, ignores it, and passes
another value to its parent continuation.

Actually, this already exists: k_seq.

Indeed:  Using a continuation like this:
         
         _ -> (fn a1 a2 (begin _ a3))

will also ignore the supplied value and reduce to (fn a1 a2 a3).

    
Entry: closures and values
Date: Sat Aug  8 12:33:11 CEST 2009

So.. It's probably best to move closure_pack to the start of step(),
and unpack before adding to the environment.  It's a bit awkward the
way it is now.

Let's make it such that:

      - on step() entry, the closure is unpacked to perform reduction.

      - if there is no reduction, the value is packed.



Entry: magic values
Date: Sat Aug  8 12:47:55 CEST 2009

Because closures and ast's have a special meaning in the interpreter,
they cannot be used as values in Scheme.



Entry: Interesting
Date: Sat Aug  8 15:28:07 CEST 2009

Implementing this Scheme interpreter without relying on cons or
closures has been most revealing.  I never knew I knew so little about
the mechanics.  Arrogance tends to get in the way of learning..

There are some meta-confusion things in the syntax representation that
are a bit fishy still:

  - Syntax is wrapped in a weird way (tagged s-expressions.  a full
    parse into an AST so values and terms can never be confused would
    probably be better).

  - Because of this confusion, primitives can return reducable
    expressions (make-ast) and closures (make-closure) that will not
    behave as literal data.

As I've mentioned before, this makes make-ast behave as eval.

Maybe the solution is to explicitly represent non-reducable values as
VALUE?

Ok: 

 * all primitive return values will be tagged with VALUE
 * REDEX that is not a varref or an application is tagged as VALUE


Ok, I see..

It's probably best to get rid of the word "closure", and have two
kinds of values that contain environments:

      - redex
      - lambda

The trouble is primitive values.  In the lambda calculus, there are
only two kinds of terms: applications and abstractions, where
abstractions are values.

So.. Can I get rid of it in one try?  The code should be a lot clearer
with this..

I think I got the concept right, but there is some stray assignment
that messes up the GC.  Maybe time to isolate all assignments?

Inspecting the run state it gives something like:

... (#k_apply(#k_macro(#k_apply(#k_macro(#k_apply( ...

Ok.. recursive macro application.

After some more small tagging bugs (sc_apply_ktx required some head
scratching) it seems to work now.  The only remaining problem is the
invokation of (gc) from within scheme.

Was a problem with sc->entries.  Added sc->step_entries, which is a
different semaphore.

Seems to work now.  I can call the interpreter step from within scheme
+ all the state objects are available:

(make-value 123)
=> #value(123)

(eval-step (make-state (make-redex 'a '((a . 123))) (mt)))
=> #state(#value(123) #k_mt)

(eval-step (eval-step (make-state (make-redex 'a '((a . 123))) (mt))))
=> #error(halt 123 #state(#value(123) #k_mt))


Entry: Loop without cons
Date: Sun Aug  9 01:09:22 CEST 2009

So..  What would be necessary to make an interpreter that can perform
a loop without performing allocation?  It would need to overwrite
variables.

So..  What is scheme without GC?  This should be split in two: Scheme
with closures, but without continuations (using a stack) vs. Scheme
without closures, or only downward closures.


Entry: Reader task
Date: Sun Aug  9 11:32:40 CEST 2009

I got distracted..  Getting a front row view of how Scheme is
implemented on top of a bare machine is fascinating.

Anyways.  Now that the core of the interpreter seems to work, let's
make the first attempt at re-usable code: a reader = tokenizer +
"compressed ast parser" for s-expressions.

A compressed ast is a binary flat representation of a tree, consisting
of a stream of words.  This is important, because it will be the main
interface between two communicating processes that do not share
memory (or not all memory: they might share constant pointers.).



Entry: Keeping track of value names
Date: Sun Aug  9 12:05:38 CEST 2009

How to tag values with names?  Every time a value is lifted from the
environment, it could be recorded in the VALUE struct for debugging
purposes.  When there is an error, the current value's name could be
used for reporting.

Hmm..  It's not so simple..  It's probably easier to tag the primitive
struct itself with names.

Ok: solution was this: tag the sc struct with the primitive before
executing it: exceptions come from the call chain, or the interpreter.


Entry: Byte code
Date: Sun Aug  9 13:40:28 CEST 2009

More procrastination.  What would Scheme byte code look like?  It is
essentially creating a cached representation of the interpreter
sequencing.

The answer seems to be CPS conversion.


Entry: More library code
Date: Sun Aug  9 18:05:10 CEST 2009

let, letrec, named-let, apply, eval, cond.


Entry: Scheme interpreter next:
Date: Mon Aug 10 09:35:13 CEST 2009

critical path:

* C tasks -> reader (also find a standard protocol for transferring
  tree data using a machine word port interface).


structured procrastination:

* compilation to bytecode (CPS)

* subset compilation to C: i.e. embedding C in s-expr syntax using
  let* for set! begin while.  maybe augmented with a bit of
  optimizations to make the subset larger (PreScheme style).

* garbage collector: the stop & copy collector is good for
  bootstrapping, but it at least needs to be replaced by a cheney
  collector that doesn't use a stack.  maybe some more fancy things
  later.  this could be abstracted out and reused.



Entry: Tasks: one-shot + continuations
Date: Mon Aug 10 11:52:16 CEST 2009

It looks like the simplest approach is to use a `prompt'.  The current
tinyscheme implementation has a transparent mechanism: any C primitive
can be suspended relative to the setjmp right before primitive
execution.  The setjmp is also used for implementing exceptions.

The scheme in this project has a different structure: there is some
state unpacking between the mark and the actual primitive call.

This makes me thing it might be better to implement c continuations
and one-shot tasks using a primitive that marks the context.

OK. Continuations are working: they create a new ck struct on each
suspend.  It's probably to make one-shot continuations if the context
is reused + a context size check is performed.

NEXT: 
  - allocate before running (run on separate stack)
  - one-shot


Entry: GC
Date: Mon Aug 10 15:48:22 CEST 2009

Apparently there's a problem with finalization of atoms: if the
unreachable garbage space contains more than one reference to an atom,
its finalization will be called twice.

Hmm..

It looks like atoms that have a free() method need to be wrapped in a
vector, to make sure they only appear once in the heap.

So, the data needs a different encoding.

Let's start from what we need in the GC:

- vectors
- constants
- integers
- finalizers

An object that needs finalization which is not idempotent needs to be
wrapped in a vector.

OK: solved by removing the 'atom' type, but requiring that every
finalizer found in the heap is followed a pointer to finalize.  This
gives a simple mechanism to build wrapper vectors around arbitrary
(aligned) pointers: the client just needs to ensure that each object
occurs only in a single vector, and use that vector as the opaque
representation of the object.


Entry: Alloc before call
Date: Mon Aug 10 17:03:12 CEST 2009

Instead of allocating before calling a primitive, in the case where
the intermediate operation doesn't alloc, it might be simpler to see
if there is enough space and then postpone the allocation, in order
not to create garbage.


Entry: 0xbad
Date: Mon Aug 10 19:31:34 CEST 2009

A coincidence: usually this indicates problems :)

1: test_ck()
(123 . #aref(#fin<0xba6270:0x40123c> #data<0xbad420>))
2: test_ck()
(124 . #aref(#fin<0xba6270:0x40123c> #data<0xbad5b0>))
2: test_ck()
(124 . #aref(#fin<0xba6270:0x40123c> #data<0xbad740>))
2: test_ck()
(124 . #aref(#fin<0xba6270:0x40123c> #data<0xbad8d0>))
3: test_ck()
125


Entry: Dybvig Danfest 2004
Date: Mon Aug 10 23:19:58 CEST 2009

"The macro writers bill of rights"[1].

Check out KFFD hygienic expansion algo[4].  This lead me to hygienic
macros on top of unhygienic ones[2].

Apparently my letrec is wrong: all evaluations need to happen before
all assignments.

Optimizations: constant folding, copy propagation, inlining, dead code
elimnation.  Inlining (deciding to inline) isn't as easy as copy
propagation: you want your compiler to terminate and prevent code
explosion.

[1] http://video.google.com/videoplay?docid=-6899972066795135270
[2] http://p-cos.net/documents/hygiene.pdf
[3] http://people.csail.mit.edu/jaffer/r4rs_12.html
[4] http://www.cs.ucdavis.edu/~devanbu/teaching/260/kohlbecker.pdf

Entry: Sequential code
Date: Tue Aug 11 09:39:04 CEST 2009

Visually I associate left-aligned code in Scheme with side-effects
(begin).  However, parallel code also looks like this, and is not
necessarily bad from a mutation pov.


Entry: GCC
Date: Tue Aug 11 10:19:25 CEST 2009

Looking at the assembly output of scheme.c and it seems that it
performs the inlining well, except for the allocation, which uses a
vararg function _sc_make_struct that looks rather expensive to call.

Also, gc_alloc won't line.  I guess the `full' check prevents that.
It's probably easier to implement GC boundaries with unmapped pages.


Entry: bootstrap in forth
Date: Tue Aug 11 10:44:02 CEST 2009

Now, in order to get to a smaller code size.  What about writing the
step function in threaded code?  Does this make at all sense?

I don't think so: the features that are used to write the interpreter
are GC's vector construct (library) and C lexical scope and C
structure member scope.  It's rather awkward to write something like
that in a combinator language.


Entry: size
Date: Tue Aug 11 11:21:40 CEST 2009

Looks like the real culprit is the primitive registration code.  This
can be done in a table.

strip scheme
ls -l scheme

# currently
-rwxr-xr-x 1 tom tom 44592 2009-08-11 11:28 scheme

# Ha!
# The prim table actually made it bigger :)
-rwxr-xr-x 1 tom tom 45912 2009-08-11 11:34 scheme

# Changing the prototype of the function to take strings instead of
# symbols made it smaller:
-rwxr-xr-x 1 tom tom 43936 2009-08-11 12:02 scheme

So it's really the code that generates the boot code that bloats the
binary.  Looks like a reader is what's needed to compress further.

Also, part of the data structures could be loaded as constants:
symbols and primitives are not GC-managed, so they could be defined as
constant data.

Another thing that leads to bloat is the tag shifting.  Can this be
simplified to a single AND + compare?

After separating the bootstrap code to lib.o i get these stripped sizes:

# gcc -m32
-rw-r--r-- 1 tom tom  1256 2009-08-11 12:48 gc.o
-rw-r--r-- 1 tom tom 16116 2009-08-11 12:48 lib.o
-rw-r--r-- 1 tom tom  2032 2009-08-11 12:48 main.o
-rw-r--r-- 1 tom tom 12456 2009-08-11 12:48 scheme.o
-rw-r--r-- 1 tom tom   676 2009-08-11 12:48 symbol.o
-rw-r--r-- 1 tom tom  1136 2009-08-11 12:48 task.o

# gcc -m64
-rw-r--r-- 1 tom tom  1904 2009-08-11 12:49 gc.o
-rw-r--r-- 1 tom tom 18040 2009-08-11 12:49 lib.o
-rw-r--r-- 1 tom tom  2640 2009-08-11 12:49 main.o
-rw-r--r-- 1 tom tom 17344 2009-08-11 12:49 scheme.o
-rw-r--r-- 1 tom tom  1176 2009-08-11 12:49 symbol.o
-rw-r--r-- 1 tom tom  1832 2009-08-11 12:49 task.o


Entry: Compressing an s-expression into a serial protocol.
Date: Tue Aug 11 12:53:48 CEST 2009

What about this: create a small stack based VM spec that can can
create a data structure from an atom stack, and send the instructions
over the wire.

Hmm.. confusing..  Really I want a proper one-shot abstraction without
copying before attempting this.

What I really want is expanded AST compression or compilation to CPS
bytecode.


Entry: CPS conversion
Date: Tue Aug 11 13:57:19 CEST 2009

(lambda (x y) (fn (- x 1) (+ y 2)))

(lambda (x y k)
  a1 <- (- x 1)
  a2 <- (+ y 2)
  (fn a1 a2 k))


Entry: Local macros
Date: Tue Aug 11 14:06:20 CEST 2009

Let's support those by also recording the macro environment.  Looks
like it's easier to first support `let' as a special form, and then
reuse that code for `letmacro'.

`let' would use an application frame, where the first value is already
filled in (a lambda created from the body).  The same could be done
with `letmacro'.

What would be a `lambda' where the names are bound to macros instead
of values?  Maybe that's the simpler route?

Time to take a break, this is confusing..


Entry: Compiling to C and pattern matching
Date: Tue Aug 11 16:48:29 CEST 2009

Take the interpreter's main loop and write it as a pattern match in
Scheme syntax.  Actually: a pattern matcher is a language for writing
interpreters.

Basicly, all the C structs, predicates, and if/then/else blocks can be
automated easily, as long as it maps data -> data.  Otoh, the C code
itself isn't so difficult.



Entry: Dynamic languages workshop
Date: Tue Aug 11 17:05:08 CEST 2009

http://www.ai.mit.edu/projects/dynlangs/wizards-panels.html

Panel 1: runtimes

David Moon:

 ``Start with a small working system and expand it incrementally,
   adding functionality and adding performance.  The big bang way only
   works for God.  Everyone else has to use evolution.''

 ``Sometimes an error message means exactly what it says.''


Panel 2: compilation

An interesting discussion about finalizers.  For libprim tasks, I'm
thinking that C++ destructors might be interesting in combination with
stack-based "pure" tasks: going out of scope due to return or
exceptions calls destructors.  Will Clinger mentions weak pointers
[1].  Also David Detlefs talked about reference counting coming back.
Figure out what he really meant.


Panel 3: Language design

Jonathan Rees: Tension between distributed programming and OO: an
  object is essentially "here": all abstractions around that is
  essentially smoke and mirrors.  (I.e.: physical location/locality
  becomes more important as systems grow physicaly larger.)  This is
  about physical things (state) vs. information.  Object != value.

Guy Steele: little known secrets

 - stacks are overrated

 - GC is good, finalizers are bad (finalizers w. good semantics <- GC
   at every pointer alteration)

 - lexical scoping is good.  easy to screw up when writing
   interpreter.

 - type inference can be good but error reporting is hard.

 - monads maybe very good

 - higher order functionals are underrated.  thinking in APL can
   approve your lisp code.

 - keyword arguments are good (PG)

 - pretty printers are vastly underrated

 - rule-based systems. (20 years ago, time to try again). 

[1] http://www.iecc.com/gclist/GC-faq.html

Entry: removing optimizations
Date: Tue Aug 11 17:45:36 CEST 2009

Heeding good advice: removing all optimizations that make code look
more complicated.


Entry: Real-time Garbage Collection
Date: Tue Aug 11 22:20:29 CEST 2009

Supercollider uses [1] while libgarbagecollector[2] is used in Io.
This one[3] is a rework of Metronome.

I'd like to know how this works..

After reading a bit, the conclusion I get to is that this isn't really
a science.  Optimality boils down to figuring out which objects are
long lived, so they can be ignored most of the time.  There are a
bunch of heuristic tricks to do this, but none of them are optimal
for all programs.

[1] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.4550
[2] http://www.dekorte.com/projects/opensource/libgarbagecollector/
[3] http://lambda-the-ultimate.org/node/2393

Entry: GC stuff
Date: Tue Aug 11 22:49:43 CEST 2009

Generational collection.

One of the key points seems to be that one mostly collects the
`nursery', a pool of recently allocated objects.  The assumption is
that most of the elements in the nursery will become garbage soon
after allocation (child mortality) AND that there are a lot of objects
that don't.  In order for this to work there needs to be a cheap way
of finding pointers that point from the heap at large into the (much
smaller) nursery.


Finalization: Instead of scanning the from space after copying, it's
also possible to perform the finalization later, when allocating anew:
the data will be moved to the cache anyway because of initialization,
so we might just as well access it then.



Entry: Cheney GC
Date: Wed Aug 12 10:13:23 CEST 2009

[ Minsky-Fenichel-Yochelson-Cheney-Arnborg (MFYCA) method ]

Like a recursive mark and copy collector, but using a breadth-first
search.

  1 Copy all root objects: copy atoms, update moved vectors and copy
    vectors not yet moved.

  2 Copy all references in newly allocated objects.

  3 Repeat until heap stops growing (all reachable objects are moved).

OK: typed in, compiling, not working: it blows up.  Simplified, but I
can't see it.  Guess it's time for a proof.

An interesting challenge this is: I've always stayed away from
implementing graph algorithms in C.  It requires a lot of discipline
to get the invariants right.  The cause is usually due to embedding
and premature optimization, i.e. using the same field to represent two
distinct values in different modes when there are subtle cases where
this is not valid.  The Cheney algo is a simple one, but I couldn't
get it right ``just writing it down''.

Ha.. Indeed: caused by leaky abstractions: the problem was that vector
tags didn't get copied, which probably triggered an allocation loop in
the interpreter due to type errors.

Next: add assertions and error handling for the whole project.



[1] http://en.wikipedia.org/wiki/Cheney%27s_algorithm


Entry: Simpler k_apply
Date: Thu Aug 13 12:17:00 CEST 2009

Next time I write an interpreter, it will be a right-to-left
evaluating one.  This simplifies the form of apply continuations, to
easier support the `apply' primitive.


Entry: Restating goals
Date: Thu Aug 13 12:27:44 CEST 2009

In libprim I'm trying to explore the idea of non-invasive scripting.
Essentially, it explores a concurrency-oriented programming style,
where one of the tasks is a `management core'.

At the same time I'm looking into simple interpreter cores to serve as
the operating system.  Currently there's a bare-bones scheme
interpreter.


Entry: Ports
Date: Thu Aug 13 12:36:27 CEST 2009

Added a ports struct (GCd aref) + a debug port until dynamic scope is
implemented.


Entry: Current scheme implementation wrt. generated code.
Date: Thu Aug 13 13:58:21 CEST 2009

Code is generated for declaring primitive functions and defining boot
code in C (as long as there is no reader).  This could be augmented by
generating all code that has to do with data structures: struct def,
lowlevel casts, and highlevel predicates and constructors.
Additionally, all primitive trampoline code could be generated.

Definitely: when the primitive struct accessors need to be available,
it would be simpler to use a code generator.


Entry: Leaf: small object model
Date: Thu Aug 13 14:37:22 CEST 2009

So..  Basicly, what's next is to make primitive data extension
simpler.  I'd like to get to a functionality of basic Scheme or PF: a
collection of primitive objects written in C, easily interfaced with
scripting code and independent of its data and control models.

For the GC memory model (Scheme), such objects are wrapped in aref
(finalizer + constant).  For the linear model, they would be wrapped
in a refcount cell.

tom@zni:~/libprim/leaf$ ls *.c
bytes.c  port.c  symbol.c  task.c


Entry: Naming conventions
Date: Thu Aug 13 16:25:49 CEST 2009

The trick seems to really be: use decent naming conventions for C
functions and types.  This way a lot of boilerplate is easily
generated at compile time.


Entry: Stack language
Date: Thu Aug 13 16:52:32 CEST 2009

With the scheme language as good as finished, let's start the stack
language.

1. references
2. cell-base
3. queues

Ok.. Separated out the object representation of GC: it can be re-used
in refcount based memory model.

The first thing to do is to create the memory and object model.
Essentially: implement `void' and `drop'.

Done.  Next: print objects.  This can be done by factoring out the
primitive print functions.


Entry: Dual regime
Date: Thu Aug 13 18:46:32 CEST 2009

I started with the basics of pf2.c : stack based alloc and free.
Maybe the important thing to realize is that it's not going to be
possible to be completely linear, due to references to code and global
storage.   However, it might make sense to get to an almost-linear
regime, where GC is active during load and setup, and after that the
GC is switched off and only the linear language runs.

Anyway, let's just be carful to make sure pf data types are compatible
with GC.

Alternatively: the code/variable space could be made inaccessible to
the language.  If it is externally managed and accessed abstractly,
references into this memory _can_ be used.  



Entry: Reusing primitives
Date: Thu Aug 13 23:16:55 CEST 2009

There seems to be a real benefit in doing this in a shared way.  All
Scheme primitives that do not CONS can be directly reused.

Even better: the whole machine might be embeddable inside scheme: this
would give a way to make references work (variables and code), which
would lie outside the reach of stack code, but could be used as jump
targets and data storage.



Entry: Moving the stack language into the Scheme interpreter.
Date: Fri Aug 14 08:48:30 CEST 2009

Basic idea: the PF machine never CONSes from the GC free space for
_data_ objects.  It's CONS is a reusing one.

What about doing this differently: separate out the memory model from
the scheme struct.  This includes GC and all leaf object classes.

The thing to decide is which objects are allowed in the stack machine
as data objects, and which should remain hidden.  There's no use for
exporting the Scheme's internal data structures (yet).  Let's start
with cons cells and atoms.  Maybe the main problem is representing
RC'd atoms in the Scheme core.

It doesn't seem to be necessary to run PF in SC.  As long as the
memory management is compatible, they can be made to work together.
The hairy part is the GC restart, since they would need to share the
GC next to all the mem classes, but it doesn't seem infeasible.

So, given that we're using the Scheme's data model, how should
finalization and reference counting be bridged?


Entry: Packet Forth
Date: Fri Aug 14 11:09:11 CEST 2009

It's becoming more clear to me that this is really a rewrite of PF.
The core idea is to be able to use linear+RC based memory management
for ``regime programs'' : code that performs some setup and then ends
up in an infinite loop with very predictable memory patterns.

The mistake I made in the first implementation is to assume that GC
isn't necessary.  Only after _first_ implementing a Scheme around the
primitive data types did I realize that this is really an essential
part: the synergy between a Scheme-like host and a linear stack
language that can mutate the GC-managed graph is essential to the
idea.  You need the first to provide "global memory" for the second.

Representing data.

The PF primitives should know about (``inside memory'').
    - constants
    - cons cells
    - RC managed leaf objects
    - opaque references to "outside" memory

The outside model, accessible from Scheme, needs to wrap RC managed
cells in an aref object.

An ``outside pointer'' TAG_OPAQUE is a data structure that is
GC-managed, but can be accessed abstractly from the inside.  It is
treated as a constant.

Basicly, in the linear model, everything could be opaque by default.
Essentially, PF doesn't need to care about managing anything else than
CONS cells and RC objects.

Now, whenever an RC value is made accessible to Scheme, it needs to be
wrapped in an aref, where the finalizer decrements the refcount.  Such
values are really meant to be kept local to the linear memory, but of
course it shouldn't be prevented to have them in the global space,
i.e. embedded in a piece of code as a constant.

This is not trivial however.  The refcounted object should be linked
to the Scheme wrapper, to keep the representation unique.  However,
this link must not become stale!

It looks like an rc struct needs to be a GC managed vector.
Conceptually it's not so hard: my difficulty is with the
representation.

        LIN -> GRAPH : rc++
        finalize:      rc--

        wrappers don't need to be unique!

The latter remark is key.

Looks like this works fine.


Entry: Code and Variables
Date: Fri Aug 14 16:26:47 CEST 2009

The VM's data structures should not be visible inside the PF language:
the machine is not reflective.  This is a key difference with the
current PF implementation.  This enables graph data to be used to
represent PF byte code.  (Note that if reflection is necessary, it can
be implemented at a greater expense by _copying_ from graph -> linear
memory and back.)

The remaining problem is then the representation of variables.
Essentially a variable is a box that's referenced by code.

            CODE -> BOX

This BOX can then be loaded on the data stack to enable the PF machine
to modify its contents.

But.  What can be put in a box?  What if a box disappears?  Hmm.. not
done yet.  Basicly it seems that a box needs to be an aref.  What if
an aref contains a list?  Yes.. plenty of pitfalls..

This can be solved by changing the semantics of finalizers: they take
objects instead of constant's pointer.  By tagging constant with 00
nothing really changes though.

So, finalizers take objects: this means that linear structures can be
embedded in an aref: if an aref becomes unreachable, an RC-- will
ripple through the list.


Entry: Scheme Primitives in the Stack Machine
Date: Fri Aug 14 18:00:40 CEST 2009

If a primtive doesn't CONS it can be reused without problems.  Let's
separate all of them out, making reuse easier.



Entry: Queues
Date: Fri Aug 14 18:01:53 CEST 2009

As the CONS of two stacks?  Note that this gives only amortized O(1)
addittions and removals, and might for that reason not be a good
candidate.  Otoh, it does allows representation as a tree.

Because of the linear nature however, mutable vector-based solutions
might work.



Entry: Packet Forth Interpreter
Date: Fri Aug 14 18:08:45 CEST 2009

Next: the interpreter.  Should be quite straightforward, only this
time I'd like to not make the mistake of not having proper tail call
handling.  For some late night zen.

One thing though: the return addresses.  What are they?

They need to be real references to guarantee a safe memory model, but
they need to be distinguishable from ordinary CONS cells.  Maybe just
add a CODE cell?  Also, this gives a very elegant way to make tail
recursion work: simply link the CDR to the head of the last word
called!

Looks like this is ``continuations for free''.

The return stack then becomes a CONS list of CODE pairs.

CODE  = RETURN | (SUB : CODE)    ;; ':' is a GC CONS
SUB   = PRIM | CODE

RS    = MT | (CODE . RS)         ;; '.' is a linear CONS

Here SUB is either a PRIM which can be invoked to modify the machine
state, or a CODE list which can be executed by saving the current CODE
cdr on the return stack and jumping to the SUB CODE.  RETURN pops the
next CODE from RS and jumps to it.

( Note that RETURN is necessary _only_ if a code definition ends on a
primitive.  If not, it's simply linked to the tail procedure. )

Essentially, the return stack itself is in this case also just a code
graph ``constructed at run-time'' : it is a callable function!

Note however that `.' is a _stack_ that's managed linearly, while the
`:' is graph managed.  I.e. the return stack can't contain loops, but
the code can.

This makes the new PF semantics complete.

HA!



Entry: pf_run()
Date: Sat Aug 15 00:04:48 CEST 2009

The basic interpreter seems to be running.  Next: exceptions and
constants (quotation).

For quotations, I'm running into the same problem as for the Scheme
interpreter: it is necessary to strictly separate language and
metalanguage: explicit quote tags are necessary to be able to quote
code.


Entry: Linear pairs
Date: Sat Aug 15 11:47:05 CEST 2009

It's probably better to use a different data type for linear pairs.
That way nonlinear pairs can be used as constants too.

Otoh: reusing the same pair structure allows primitives to be reused.
Even those that do cons. (Cons could be part of a shared object).

It's really not such a hurdle to manage the border between linear and
graph memory, so let's stick to the simple implementation: cons lists
are cons lists, they just have only a single reference in the linear
part.

Entry: More sharing between Scheme and PF
Date: Sat Aug 15 12:02:16 CEST 2009

There is already quite some duplication, mostly in exception handling,
GC, and printing.  This should be handled by "mem". (mom?).

I guess it's a good idea to factor out as much as possible beforehand.

Let's rename "mem" to "base", which implements:

      - GC restart
      - primitive leaf types
      - primtiive exception handling
      - printing

Hmm..  Let's not.  The simplest way is to make sure the primitives
themselves will take from the base class.  This can be factored out
later.. It was only a couple of lines to add the ABORT exceptions in
PF + it's slightly different since the interpreter is not re-entrant.


Entry: Code bootstrap
Date: Sat Aug 15 12:48:20 CEST 2009

Now that default abort works it's time to start adding some primitives
and a bootstrap reader (sexp -> C).

Seems straightforward once list -> code translation works.  (eval).

Before going there, I think it's best to do the shared primitives
anyway.  The PF core can benefit greatly from _expressions_ : in fact,
almost all code in pf.c is doubly done: once as expression, and once
bound to the linear stack.


Entry: EX: expression language
Date: Sat Aug 15 13:55:38 CEST 2009

`expressions' use the following resources:
              - GC
              - exceptions
              - leaf types

It is a dynamically typed functional language using the C-stack for
expression nesting.

The Scheme and PF languages add primitives that operate on an
extension of this state, with most of the primitive expressions
shared.

Now, by binding EX in the respective scripting languages to &pf->m and
&sc->m it is possible to abbreviate all expressions, leaving out the
prefixes.  I.e. generalization of:

#define CONS(a,b)  ex_cons(EX, a, b)

Now that the infrastructure is there, let's move the primitives lazily
depending on need in both languages.

Entry: Static scope.
Date: Sat Aug 15 15:41:17 CEST 2009

What's interesting for the current PF code graph implementation, is
that it is actually statically scoped.  All variables are bound at
compile time, which makes (data / code) hiding through local
definitions possible.

I'm not sure how to reflect this back into the language, but it could
be used to create local dictionaries, one of the things that was
really hard to do in original PF.


Entry: PF / PX primitives
Date: Sat Aug 15 16:05:08 CEST 2009

In light of this sharing it makes sense to define PF in terms of PX
primitives: in-place expressions.

pf_  forth words
pl_  linear (in-place update) expressions

ex_  nonlinear (sharing / consing) expressions (usable in Scheme)



pf_  probably are redundant, and can be generated from px_


Entry: EX printing
Date: Sat Aug 15 16:43:05 CEST 2009

EX has a lowlevel port* reference to implement basic printing.  It
needs a lowlevel interface because in SC and PF ports are managed
respectively as graph and linear atoms.


Entry: Building
Date: Sat Aug 15 20:47:04 CEST 2009

Looks like I've got the build problems ironed out.  Moral: dont _EVER_
generate files outside your local makefile directory.  This can be
subtly done by some rule that depends on a specification outside of
your dir..  I had something like this:

../ex/ex_prims.h_:  ../ex/ex_prims.c

which nicely overwrite the .h_ file that should have been built in
another directory using a locally defined rule for .h_ files

Another thing is to use different extensions for different generators.
Otherwise the problem above is bound to impose itself.


Entry: EX errors
Date: Sat Aug 15 21:18:47 CEST 2009

So.. Time to move primitive errors from SC -> EX.
Done.


Entry: Knot tying
Date: Sat Aug 15 22:50:25 CEST 2009

To define recursive functions, it is necessary to use mutation of the
code graph.  I would like to preserve a semantics of early binding
though.

OK, it's not so difficult.  Adjust the environment so each pair points
to a code cell.  This is the one that will be referred to from within
other code.  When a definition arrives, reuse the cell.

What I really would like is a way to call Scheme code during
definition.  Mutation doesn't go well with the allocations necessary.
So maybe this needs to be written as a bootstrapped composite word?

Otoh, it might be solved linearly, translating the code in-place,
transforming the structure into code directly.

1. compile in-place, reusing the cons cells (code is isomorphic to the
data representation, except for `quote' and variable wrappers.

2. update the global environment.

This would work were it not for the wrapping that needs to perform
allocation.  It really would be simpler to do it from Scheme.

Alternatively, it can be done in two passes: one pass creates the
skeleton but doesn't mutate to make sure it can be restarted, while
the second pass fills in the values.

Also, it's possible to start with a non-recursive version and use that
to boostrap the recursive one as a primitive.

It's really not so difficult: the only need is that everything fits
into free GC space before the modification to the environment is made.


Entry: PF compiler
Date: Sun Aug 16 11:44:20 CEST 2009

"Compiler" is a strong word: it's mostly a linker.  As mentioned in
the previous post, this is a bit awkward due to the graph patching
that needs to be done.  There are many corner cases related to
singleton words (that need ``eta-reduction'') or primitives, that can
be inlined.

The real question is (and I've run into this before) does the
dictionary contain CODE references, or SUB references?

   CODE  = RETURN | (SUB : CODE)    ;; ':' is a GC CONS
   SUB   = PRIM | QUOTE | CODE
 
   RS    = MT | (CODE . RS)         ;; '.' is a linear CONS

It might make more sense to use SUB refs.  Tail calls are tricky!

Let's change the types a bit:

   CPAIR = (SUB : CODE)
   CODE  = RETURN | CPAIR   
   SUB   = PRIM | QUOTE | CPAIR
 
   RS    = NIL | (CPAIR . RS) 

The reason this feels a bit weird is that composite code doesn't need
RETURN.  The only way to ever pop RS is to have a word end in a
primitive, i.e. [ PRIM RETURN ].  Composite code can always just jump.

Looking at CPS versions of Scheme, this is also quite clear: the only
thing that ever _invokes_ the continuation, is a primitive.

So, can the types be made a bit more intuitive?



Entry: New PF interpreter
Date: Sun Aug 16 12:09:57 CEST 2009


The interpreter evaluates the following recursive data structure.

( Note that this represents fully resolved byte code: it doesn't contain
  any variable references. )

      SEQ = (SUB : SUB)            ;; `:' is graph CONS
      SUB = PRIM | QUOTE | SEQ
 
      K   = HALT | (SUB . K)       ;; `.' is linear CONS

SEQ  Code sequencing provides the encoding of what to do _now_ and what
     to do _next_

SUB  A subroutine is a basic unit of work: invoke a primitive function,
     load a data object onto the parameter stack, or perform a
     code sequence.

K    The continuation is a stack constructed at run-time, representing
     the rest of the evaluation, finishing with HALT.


The interpreter uses this data structure as follows (in pseudo code,
an ML implementation can be found here[1]).

      sub = POP(K)
      case sub
         SEQ (now : next)  
           -> K = PUSH(now, PUSH(next, K))
         PRIM
           -> execute PRIM
         QUOTE
           -> load data item on parameter stack

SUB is the main code type (the subroutine).  It is what you pass to
`run', and it is what can be found as resume points in the
continuation.  It includes primitive code and data, as well as
composite code sequences (SEQ).

When a SEQ is encoded, the two SUBs will be pushed onto K in the
correct order.  Note that this encoding automatically gives proper
tail call handling[2]: loops can be implemented using recursion
without blowing up the K stack.


                            * * *

The above is the representation of compiled code.

Concatenative (Joy-like) code can be compiled straightforwardly to
SEQ-based code.  This can be illustrated using the standard lisp
CDR-linked pair notation to represent both source code CONS pairs and
SUB pairs.  The the symbolic program

   (a b c d)     = CONS(a, CONS(b, CONS(c, CONS(d, NIL))))

is compiled to an improper list 

   (A B C . D)   = SEQ(A, SEQ(B, SEQ(C, D)))

where each symbol x is mapped to a corresponding X representing a SUB.
Resolution happens using a global vocabulary that maps symbols to
SUBs.
 
                            * * *

The code representation as a binary graph has several advantages
related to the similarity between binary graphs and binary trees (PF's
linear pair data structure for data storage and representing
the machine continuation).

    - SEQ as a pair of SUBs gives a very direct encoding of "now" vs
      "next".  This makes it really easy to represent continuations as
      a stack of SUBs.

    - The continuation K ``looks like'' a SEQ (a particular kind of
      SUB), in that it is a linear pair of a SUB and a K.

    - The machine can run without nonlinear data allocation:
      continuation push/pops are linear.

    - One-shot full and partial continuations (segments of the K
      stack) are (isomorphic to) linear lists.

    - This isomorphism obviously works also the other way: linear
      lists constructed at run time can be converted to one-shot full
      or partial continuations.  (This can be used to emulate one-shot
      `closures'.)

    - Linear one-shot closures/continuations can be _copied_ to make
      them multi-shot.  

    - They can also be _compiled_ to code (linear pairs represented by
      nonlinear SEQ pairs) so they can be reused any number of times.
      However, this is not a linear operation.


( Note that the isomorphisms need some work to figure out quoting
issues.  Essentially it will boil down to different kinds of
continuation frames. )



[1] http://zwizwa.be/darcs/libprim/pf/pf.ml
[2] http://repository.readscheme.org/ftp/papers/ai-lab-pubs/AIM-443.pdf




Entry: Compilation
Date: Sun Aug 16 12:43:45 CEST 2009

Now.. I think the bytecode does what I need it to do.  But how to
compile it?  This requires multiple passes to properly inline all
references.

I have something like this:

FOR(SRC):
        symbol? YES -> dereference if possible
                NO  -> quote as datum

        next?   YES -> alloc SEQ and continue
                NO  -> store and finish

Then iterate over the data structure until all symbols are
dereferenced.

There are (at least?) two special cases that won't work:

 - empty programs are not handled properly.

 - degenerate loops lead to infinite dereference passes:

   (a b)
   (b a)


Handling those separately should make it work though.  First, filter
out all references to empty programs.  Second: use a counter to
indicate the number of dereferences performed.  If 0, there is no
progress: any remaining references are degenerate loops.

The empty program (NOP) is necessary as a corner case, because empty
quotations need to work.  These can be optimized out of normal code
though.

Hmm.. this approach has a number of pitfalls.  It isn't possible to
determine whether a program is a NOP before completely compiling it.

Let's make it work first using a 1->1 map from source structure, and
perform optimizations in a second pass.

So, the passes are:

  (PURE)
    1. construct environment
    2. compile s-expr -> AST  (``improper'' SUB lists)
    3. resolve all references
    4. check degenerate loops (NI)
    5. snap pointers (NI)
  (IMPURE)
    6. patch global environment (NI)

Most of the passes can be done purely.  They don't create much garbage
so they can be kept as primitives (should make it to the end without
GC).

To test it, first the primitives need to be defined.

OK. Prims load + highlevel code compiles.


Entry: Linear cons cells
Date: Sun Aug 16 18:46:55 CEST 2009

It might be best to switch to an explicit representation for linear
CONS cells.  Especially for the interaction with Scheme, it might be
confusing to have the same datastructure represent two different
things.

The hack is of course that inside the PF primitives, it's possible to
use direct calls to read-only ex_ functions, without the need to make
them polymorphic to both linear and nonlinear lists.

Anyways, this seems something to postpone until it becomes a problem.
It should be straightforward to do: it's mostly the functions
_pf_copy_from_graph and _pf_copy_to_graph.



Entry: QUOTE
Date: Sun Aug 16 19:32:48 CEST 2009

Another premature optimization: currently there's a lot of copying
going on when defining new code from within PF (represented as linear
lists).


Entry: Decompile
Date: Sun Aug 16 20:59:37 CEST 2009

With everything packed as a graph, it's quite difficult to print out
any code.  Let's add some reverse lookup for code printing.

OK.  This seems to work.  With a bit of effort re-quoting also works.
Note that words don't have a definite end, except when they end in a
primitive.  Whenever it encounters a sequence tail that has a name, it
prints that instead.  Using brackets for printing code:

(def . ['456 dup-post abc])
(loop . ['1 [print-state] ['1 '2] loop])
(abc . ['123 dup-post])


Entry: Autosnarf
Date: Sun Aug 16 22:38:53 CEST 2009

From EX / PX -> PF.

Possible?  Probably not, since there are no guarantees about linearity
of the output.  Without proper typing this doesn't seem to work..

So it needs to be done manually.  



Entry: No IP
Date: Mon Aug 17 09:18:48 CEST 2009

With the machine organized as it is now, there is really no need for
an IP register: always use the top of RS.

Well, conceptually that's the case, but let's keep it like it is now.
It's an implementation issue that doesn't have too many consequences.

Another thing though: the return stack can't be used for storing
temporary data.  Or can it?  Looks like it can, at least in the middle
of a SEQ, because an atom placed on RS will only be accessed when
evaluating a primitive or constant in tail position.


Entry: Next?
Date: Mon Aug 17 09:55:03 CEST 2009

- More primitives: combinators + list/stack/queue ops.
- Reader


Entry: Making PF expressions available to scheme.
Date: Mon Aug 17 09:59:05 CEST 2009

The requirement is to decouple them from the pf* state other than ex*.
Very straightforward: rename them px_ -> ex_ and use the
../ex/ex_prims.ss snarfer.


Entry: Does PF need real-time GC?
Date: Mon Aug 17 10:27:47 CEST 2009

Note that I realize that these days it is possible to get real-time
incremental GC.  However, these are still not synchronous memory
managers: they won't reclaim large blocks immediately so always
require more memory than a synchronous GC, and on top of that will
cause unnecessary loss of data locality.

This doesn't mean that the current (very simple stop-and-copy Cheney
GC) can't be replaced by something more elaborate to make the
_compilation_ also respect real-time constraints.

Otoh, since the linear and graph memory are completely separate, and
the GC will never modify the linear data, a GC that doesn't move the
data can be run in a low priority thread without problems.

So, yes: I guess I'm taking position _against_ real-time GC, at least
for scripted DSP applications (for static compilation, you can
probably use static buffer allocation) where large chunks of memory
are allocated and freed at a rapid rate.  It seems that the
multitasking approach makes more sense: a linear core which can use
(read-only) data and code from a nonlinear one.

... Baker 1978[1] -> contains some information about CDR coding.
Useful for making byte code sequential (cache-friendly).  Chapter 8 is
of interest though: talks about RC counting.

The trouble with RC managed data that Baker mentions is that a `drop'
of a huge data structure takes a long time to deallocate.  He suggests
to do te deallocation lazily.  In practice though (especially due to
the `physical' nature of linear objects) this doesn't really happen
much.  Consumption rate is usually one-by-one.  And, lazy dealloc
still doesn't perform fast reclaim for cache locality, what we're
really after..


[1] http://home.pipeline.com/~hbaker1/RealTimeGC.html


Entry: `Harvard' PF in Flash ROM
Date: Mon Aug 17 19:15:18 CEST 2009

Now.. If this works, does it make sense to try to implement the same
algorithm on a PIC18 while using the RAM as linear memory, and keeping
the ROM GC managed?  It really doesn't need all that much space, and
the self-write seems to be compatible.


Entry: PEG/LEG
Date: Tue Aug 18 08:58:30 CEST 2009

Trying out this[1].

The best way to understand is the metacircular example: The grammar
for peg grammars is shown below.

           Grammar         <- Spacing Definition+ EndOfFile

           Definition      <- Identifier LEFTARROW Expression
           Expression      <- Sequence ( SLASH Sequence )*
           Sequence        <- Prefix*
           Prefix          <- AND Action
                            / ( AND | NOT )? Suffix
           Suffix          <- Primary ( QUERY / STAR / PLUS )?
           Primary         <- Identifier !LEFTARROW
                            / OPEN Expression CLOSE
                            / Literal
                            / Class
                            / DOT
                            / Action
                            / BEGIN
                            / END

           Identifier      <- < IdentStart IdentCont* > Spacing
           IdentStart      <- [a-zA-Z_]
           IdentCont       <- IdentStart / [0-9]
           Literal         <- ['] < ( !['] Char  )* > ['] Spacing
                            / ["] < ( !["] Char  )* > ["] Spacing
           Class           <- '[' < ( !']' Range )* > ']' Spacing
           Range           <- Char '-' Char / Char
           Char            <- '\\' [abefnrtv'"\[\]\\]
                            / '\\' [0-3][0-7][0-7]
                            / '\\' [0-7][0-7]?
                            / '\\' '-'
                            / !'\\' .
           LEFTARROW       <- '<-' Spacing
           SLASH           <- '/' Spacing
           AND             <- '&' Spacing
           NOT             <- '!' Spacing
           QUERY           <- '?' Spacing
           STAR            <- '*' Spacing
           PLUS            <- '+' Spacing
           OPEN            <- '(' Spacing
           CLOSE           <- ')' Spacing
           DOT             <- '.' Spacing
           Spacing         <- ( Space / Comment )*
           Comment         <- '#' ( !EndOfLine . )* EndOfLine
           Space           <- ' ' / '\t' / EndOfLine
           EndOfLine       <- '\r\n' / '\n' / '\r'
           EndOfFile       <- !.
           Action          <- '{' < [^}]* > '}' Spacing
           BEGIN           <- '<' Spacing
           END             <- '>' Spacing

Starting with this:

Grammar         <- Spacing Identifier+ EndOfFile         
Spacing         <- ( Space / Comment )*
Space           <- ' ' / '\t' / EndOfLine
Comment         <- ';;' ( !EndOfLine . )* EndOfLine
EndOfLine       <- '\r\n' / '\n' / '\r'
EndOfFile       <- !.


Identifier      <- < IdentStart IdentCont* > Spacing    { printf("ID: %s\n", yytext); }
IdentStart      <- [a-zA-Z_]
IdentCont       <- IdentStart / [0-9]

Which doesn't do anything..  I guess I'm not getting
something... Maybe what I need is leg instead of peg?  Indeed, it
seems to support expressions.

exampeles/calc.leg:


%{
#include <stdio.h>
int vars[26];
%}

Stmt	= - e:Expr EOL			{ printf("%d\n", e); }
	| ( !EOL . )* EOL		{ printf("error\n"); }

Expr	= i:ID ASSIGN s:Sum		{ $$= vars[i]= s; }
	| s:Sum				{ $$= s; }

Sum	= l:Product
		( PLUS  r:Product	{ l += r; }
		| MINUS r:Product	{ l -= r; }
		)*			{ $$= l; }

Product	= l:Value
		( TIMES  r:Value	{ l *= r; }
		| DIVIDE r:Value	{ l /= r; }
		)*			{ $$= l; }

Value	= i:NUMBER			{ $$= atoi(yytext); }
	| i:ID !ASSIGN			{ $$= vars[i]; }
	| OPEN i:Expr CLOSE		{ $$= i; }

NUMBER	= < [0-9]+ >	-		{ $$= atoi(yytext); }
ID	= < [a-z]  >	-		{ $$= yytext[0] - 'a'; }
ASSIGN	= '='		-
PLUS	= '+'		-
MINUS	= '-'		-
TIMES	= '*'		-
DIVIDE	= '/'		-
OPEN	= '('		-
CLOSE	= ')'		-

-	= [ \t]*
EOL	= '\n' | '\r\n' | '\r' | ';'

%%

int main()
{
  while (yyparse());

  return 0;
}


One problem though: it doesn't seem to be possible to pass context
into yyparse that can trickle down to the actions.  Makes me wonder
how one would implement dynamic scope in C in a thread-safe way.  I
guess by using thread-local variables..

Ok.  I think I get the basic idea.  Now the subtleties.  How to
express delimiting?

[1] http://piumarta.com/software/peg/peg.1.html


Entry: Weird error
Date: Tue Aug 18 15:37:32 CEST 2009

(define (read-file)
  (let loop ()
    (let ((expr (read)))
      (if (eof-object? expr)
          (post expr)
          (begin
            (post expr)
            (loop))))))
(read-file)

Passing the string ' 1 2 3 abc "def" foo'  gives:

READ-TEST
;; gc 2340:12378
;; gc 2383:12335
;; gc 2380:12338
1
2
;; gc 2377:12341
3
abc
"def"
foo
;; gc 2298:12420
ERROR: undefined: expr

This `undefined' message should not happen, as the variable should be
in lexical context.  Something goes wrong here..

The environment does have `expr' :

(gdb) p ex_post(sc, _ex_map1_prim(sc, ex_car, env))
(expr loop)

So maybe the two symbols are not equal?

(gdb) p term
$6 = 6561248

(gdb) p ex_caar(sc, env)
$7 = 6561248

No, they are..

Ok, I know what this is:  This should use find_slot istead of find,
because the value can be #f


Entry: First PEG grammar
Date: Tue Aug 18 16:41:23 CEST 2009

Next    = - d:Datum                      { ob = d; }
        | - e:EOF                        { ob = e; }
        | j:Junk                         { ob = j; }

Datum   = List | Number | Symbol | String

LP      = '(' -
RP      = ')' -
Dot     = '.' Space -

List     = LP Tail
Tail     = a:Datum Dot d:Datum RP        { $$= CONS(a,d) }
         | RP                            { $$= NIL }
         | d:Datum t:Tail                { $$= CONS(d,t) }

Junk    = < .* > EOF                     { $$= JUNK(yytext) }

Number	= < Digit+ > !Letter -	         { $$= NUMBER(atoi(yytext)) }
Symbol  = < Letter Char* > -	         { $$= SYMBOL(yytext) }
String  = ["] < ( !["] Char )* > ["] -   { $$= STRING(yytext) }


Letter  = [A-Za-z]
Digit   = [0-9]
Char    = Letter | Digit

Space   = [ \t\n]
-	= ( Space | Comment )*
EOF     = !.                            { $$= EOF_OBJECT }

Comment = ';' ( !EOL . )* EOL
EOL	= '\n' | '\r\n' | '\r'


After adding some special characters, it parses the boot.scm file.
Parse error reporting is going to be spartan it looks like..

So, how to parse from a different input port?
( Simple: read the man page. )

I think I get it now: a PEG is a backtracking program with a
sequential left to right choice operator.  It is not a description of
a CFG.

So, in order to read a minimal amount of characters, the current
grammar needs to be reworked a bit.  Esp. the greedy post-whitespace
parse needs to be replaced by something else.

Start with simply putting all '-' rules in front.  This seems to
almost work.. The .scm file won't parse though.

Entry: REPL working
Date: Tue Aug 18 20:07:50 CEST 2009

# This is the non-greedy version of the grammar:
# http://zwizwa.be/darcs/libprim/ex/sexp.leg


Read     = - n:Next                       { ob = n; }

Next     = Datum | EOF | Junk

Datum    = Quote | Const | List 
         | Number | Symbol | String

List     = LP Tail
Tail     = a:Datum Dot d:Datum RP         { $$= CONS(a,d) }
         | RP                             { $$= NIL }
         | d:Datum t:Tail                 { $$= CONS(d,t) }

Quote    = - ['] - d:Datum                { $$= QUOTE(d) }

Const    = True | False

LP       = - '('
RP       = - ')'
Dot      = Space - '.'
True     = - '#t'                         { $$= TRUE }
False    = - '#f'                         { $$= FALSE }

Number	 = - < Digit+ > !NonDigit         { $$= NUMBER(atoi(yytext)) }
Symbol   = - < Char+ >  	          { $$= SYMBOL(yytext) }
String   = - ["] < ( !["] . )* > ["]      { $$= STRING(yytext) }


Letter   = [A-Za-z]
Digit    = [0-9]
Special  = '!' | '-' | '?' | '*' 
         | '_' | ':' | '<' | '>' 
         | '=' | '/' | '+'
NonDigit = Letter | Special
Char     = Digit | NonDigit

Space    = [ \t\n]
-        = ( Space | Comment )*

Comment  = ';' ( !EOL . )* EOL
EOL	 = '\n' | '\r\n' | '\r'

EOF      = !.                             { $$= EOF_OBJECT }

Junk     = < .* > EOF                     { $$= JUNK(yytext) }



Entry: Next
Date: Tue Aug 18 21:26:25 CEST 2009

Scheme: 
- dynamic parameters
- dynamic wind
- partial continuations

PF: 
- separate linear pair type (OK)
- export non-linear part as EX (OK)
- polymorphy
- packet reuse
- exceptions

EX:
- primitives



Entry: GC and read -> need C suspensions
Date: Wed Aug 19 08:43:40 CEST 2009

Because ports are stateful entities, the read operation might interact
badly with GC restarts.

It might be necessary to construct an intermediate representation.
The real solution however is to make the reader communicate externally
at everything which is now a CONS, so it never sees scheme values.

I guess this can be made to work, so I'm going to stick to the current
version, since that change would be transparent once context switching
is implemented.



Entry: PF compiler: SC < PF ?
Date: Wed Aug 19 09:25:54 CEST 2009

Since it needs to construct the right data types, which ultimately tie
into the memory manager through finalizers, it looks like the linear
memory manager needs to be part of the Scheme's machine state if we
want to create linear code..

So no, this doesn't work..

What might work is to do it the other way around: have PF state be an
extension of SC state, so all Scheme code can run on the PF machine,
_and_ use the PF compiler words, but not vice versa.

Indeed: the `sc' struct only has the global registers and symbol cache
in addition to the `ex' struct.  This can be easily changed.

In order for this to work, the linear data needs to be well
separated.  Let's make a separate LCONS data structure.

Ok, almost done.  ex_bang_reverse was not polymorphic, and it wasn't
completely clear where to do the graph -> lin conversion.

NONLIN -> LIN conversion should happen in pf_ functions, while px_
functions operate on nonlinear structures.  They can depend on linear
structures, as long as they don't link them to anything.  I.e. inplace
update is allowed, and some kinds of folding.


Entry: LIN vs. BOX
Date: Wed Aug 19 12:24:51 CEST 2009

It's probably simpler to place ALL linear data that is supposed to go
into the nonlinear tree in a BOX.  LIN is only useful for quotations,
which can be nonlinear values.

This does break LIN->GRAPH->LIN equivalence, since all data will be
wrapped.

Something is not well specified in the data conversion: how to handle
lists at compilation time?  The interpreter should be dumb: it unwraps
LIN, but will simpy copy anything else.

Let's default QUOTE such that it will place a LIN-wrapped copy of the
quoted datum.


Entry: GC restarts
Date: Wed Aug 19 13:04:24 CEST 2009

When this keeps growing, it's probably going to be a good idea to get
rid of the GC restarts, and scan the C stack for references to update.
Restarting works well for simple functions, but with more complicated
C functions and these functions calling each other, the invariants (no
alloc after mutation) are not so easy to maintain.


Entry: PF read needs to be linear
Date: Wed Aug 19 13:15:28 CEST 2009

This is going to be more difficult to manage.  But I/O really needs to
be free of GC issues.  Ports also need to be lowlevel (fd instead of
FILE).



Entry: PF design choices
Date: Wed Aug 19 13:55:15 CEST 2009

So what problems does it actually solve?  The PF design allows the
combination of:

    * Synchronous finalization.

    * Reflection.

In PF, synchronous finalization is a consequence of the use of
primitive functions that operate on a linear core memory: linear
lists/trees and open/close RC-managed leaf objects).

Reflection -- here limited to compilation: the translation of source
code represented as a data object to a representation of composite
functions that can be executed by the interpreter -- is made practical
by embedding the linear core machine in a non-linear dynamic
language.

This composite code is naturally represented as a non-linear data
structure: it uses sharing (subroutine calls) and can contain cycles
(program loops).

While compilation itself is a nonlinear operations and might produce
garbage that needs to be collected asynchronously, running the
resulting code does not, as it only sequences linear operations.  Note
that nonlinear data can be treated as constant by the linear
primitives and interpreter because it is managed elsewhere, as long as
the asynchronous GC traces all the nonlinear references in the linear
memory.

Dynamic features are helpful for debugging, testing and evolving
design (= writing throwaway code).  This, however, is largely a matter
of taste.  The usefulness of the linear memory manager has some deeper
reasons though.

Asynchronous GC doesn't work well in combination with finalizers:
functions that free external resources (different from core memory)
when their representation gets collected.  

In practice, an ``external resource'' is anything with an
open/use/close access semantics that _you cannot change_ for whatever
reason.  This can be anything.  Some examples:

   - A file in a filesystem.

   - A hardware resource.
   
   - Cache-managed resources (i.e. processor memory cache).

( The latter case might be unclear.  Here the open/close is not
  explicit, but some resource will be evicted whenever a resource
  that's not in the cache is accessed.  A cache doesn't behave as
  memory because there is no `memory-full' notification when an
  eviction happens.  Absence of this signal is actually the basic
  idea: it frees the user from having to ``pool'' the resource.  In
  some cases however (essentially due to broken design, when you can't
  bypass the cache API), you want to regain control over the pooling
  behaviour by manipulating the access patterns from an external
  pooling mechanism.  Note that in general, this is only a performance
  problem: dead references will eventuall get evicted from the cache
  because they are never accessed, but they might trigger eviction of
  _other_ components that are still in use.  I.e. the round-robin
  access pattern typical for an asynchronous GC does exactly this in
  the case of an LRU cache strategy. )

The problem is that you never know when the GC runs, which might lead
you to cases where you're waiting on the availability of a resource
that's unreachable, but not yet collected.  Triggering local GC in
this case isn't a solution: the reference could be held by an
unreachable representation object in a _different_ program that uses
asynchronous GC which doesn't expose an interface to its GC trigger.
The real problem here seems to be that asynchronous GC is essentially
a _global_ operation, and making this work with low-level resource
management requires a specific kind of resource -> GC signalling to
happen between all components involved.

In stark contrast, the open/close semantics composes easily (a
constructor/destructor calls several constructors/destructors), and
shields the lower layers from what happens above.  This means that in
practical situations, open/close resource semantics are the only
available option, and in the case where fast reclaim is necessary, a
linear container memory might be the simplest solution.

A linear _concatenative_ language gives you many of the benefits of a
higher level safe language, with the added benefit of not needing an
asynchronous GC, at the expense of giving up the ``random access''
that come with lexical variables.

Of course, it is also possible to translate this to a language _with_
lexical variables as long as proper constraints are placed on variable
references: every variable needs to be referenced exactly once.  This
can be enforced statically.  This seems to be harder to get right
though (i.e. use-once closures).  However, if such a language can be
defined, it can probably be directly translated into the PF
concatenative execution model.

Note that Real-Time GC (incremental GC with aribrarily low upper
bounds on collection times) isn't a solution to the problem unless it
can manage _all_ resources in a bounded time.  Collection still isn't
synchronous, so any depletion event needs to propagate into the
collector, which might break the upper collection bounds as it
requires a full GC.


Entry: Static linearity checks
Date: Wed Aug 19 14:35:50 CEST 2009

It would be nice to find a way to statically enforce the linearity
constraint.  Currently, it's quite error-prone to write primitives.


Entry: Merging SC and PF
Date: Wed Aug 19 15:16:50 CEST 2009

Maybe it's time to completely unify the two interpreters.  Essentially
this would work as a `scheme-eval-step' in PF.

The idea is that once you have everything inplace to implement PF in
EX, the only thing that's left is the Scheme interpreter's (pure) step
function and some other operations on the interpreter state.

The only problem is Scheme's toplevel environment.  However, this
could be hidden as dynamic scope in the continuation chain inside the
state.


Entry: Multiple dispatch
Date: Wed Aug 19 16:46:09 CEST 2009

How is this best implemented?


Entry: No >r, nor r> : towards call/cc
Date: Wed Aug 19 17:56:53 CEST 2009

These don't work any more.  The problem is that there isn't really a
return stack.  There is only a continuation.

If this is the case then it is better to get rid of IP, and always use
the top of RS as the continuation.

What can be done is to modify the continuation such that an operation
can be inserted after another one.  I.e. to implement `dip'.

OK: removed IP register.  RUN is now simpy a push to RS.

Let's rename RS to K.

Now call/cc is very simple:

void pf_call_with_cc(pf *pf) {
    _ fn = TOP;
    _ k = pf->k;
    pf->k = LINEAR_CONS(fn, HALT);
    _TOP = k;
}

Note these are linear one-shot continuations, but they can be copied
without causing any trouble.  They are also _data structures_ and
cannot be applied like a function.

But, maybe "run" should take linear lists, by pushing them onto K.

Note this is problematic without partial continuations (which should
be the default..)



Entry: Lowhanging fruit
Date: Wed Aug 19 20:03:57 CEST 2009

- dip

- map

- linear lists (partial continuations) should not be recognized by
  `run' -> use `run-list' or so.


Map: This can't be a primitive, but I prefer to make it fast (local),
which could be done by modifying the continuation from the primitive.

It needs 3 storage locations: in out fn

  - move result to OUT
  - replace with next from IN
  - push FN to K

What I really need is a better way to express all these stack
permuations.

Dip: there's a problem here: quote isn't a linear atom, so it should
be handled by undip.



Entry: Delimited Continuations
Date: Thu Aug 20 09:27:08 CEST 2009

In contrast with the previous PF implementation, which was mostly an
indirect threaded Forth, the current VM has a direct representation of
continuations.  Partial continuations are not so difficult to
construct on top of this: they simply splice the current K upto a
marker.

I prefer to use shift/reset, so let's implement that first.

Seems to work:

(shift 1 2 3) reset
ps
<1> [{'1 '2 '3}]
run
ps
<3> 1 2 3

The first paper about delimited continuations seems to be this
one[1].  In the introduction it mentions that the crucial idea is to
represent continuations as functions that can be composed.
In contrast, traditional continuations cannot be composed, as they do
not return a value to the calling context (it is discarded).

On page 7, on changing the CEK machine: ``One solution is changing
(CEK7) so that the invoked continuation is concatenated to the current
one: ...''  Apparently this is problematic for the denotational
semantics (continuations being opaque?) but I see no problems on the
implementation side.

Essentially, in PF this concatenation could be made ``lazy'' (just
like SEQ SUBroutines) by allowing K to be a tree instead of a list.
That way the interpreter can unpack branches one cell at a time.
Otoh, it seems simpler to place the burdon in `run' as it is now.

There are some other problems with the current implementation though:
the `prompt-tag' and `undip' functions are special, but can occur in
lists that are representing a continuation.  Therefore it is probably
best to represent the continuations abstractly to prevent programs
from destructing them.  In any case, it could be left as is now.  Just
keep in mind: a list of code can be converted to a continuation, but
vise versa isn't really sound.

[1] http://www.ccs.neu.edu/scheme/pubs/felleisen-beyond.pdf


Entry: Error prone
Date: Thu Aug 20 10:01:05 CEST 2009

As mentioned before, writing the PF primitives is error prone.
Linearity constraints are difficult to maintain in the presence of
primitive exceptions.  There is only one remedy: never keep linear
variables on the C stack.  But this can be tedious..


Entry: Pattern matching for EX
Date: Thu Aug 20 11:28:14 CEST 2009

This really shouldn't be so difficult: from the data structures, it
would be a generation of a couple of CPP macros that setup a lexical
environment.  It would require some trickery with curly braces though.

MATCH(v) 
   CASE(pair, a, d) { }
   CASE(NIL)        { }
   ELSE             { }
END_MATCH


where MATCH sets up the first '{' and each CASE starts with a '}'.


Entry: Scheme -> EX compilation
Date: Thu Aug 20 15:24:19 CEST 2009

This seems really straightforward.  It would be nice to be able to
statically check at least names and argument arity.

About writing code generators: you _really_ want to write it as a
coroutine (a `generator') that passes strings to an output port
through a `yield' function, in this case traditionally called `emit'.
Using mapping might be more elegant, but in this case due to small
differences in tree structure, decoupling the collector from the
enumerator is best.

In C, there are statements and expressions.  For EX it's simpler to
get rid of the statements early on, so one can focus on the
expressions.  Also, they don't need to be converted to statements
(CPS) since C's expression syntax will do just fine.

Then, it's simpler to use syntax-rules style pattern matching to write
the generator than an using scheme/match at runtime.

A bit of work trying to get the basic abstractions right: statements
emit a line, expressions use string substitution to be built
recursively, function/constructor names get mapped, context arguments
get added.  Everything is a function, except the language forms
themselves, which are implemented as macros (`def', `block' and `return').


(def (bar x y)
  (block ((a 123)
          (b 345))
    (block ((c (plus a b))
            (d (min a)))
       (return (foo c d x y)))))

=>
_ ex_bar(ex *ex, _ x, _ y)
{
    _ a = 123;
    _ b = 345;
    {
        _ c = ex_plus(ex, a, b);
        _ d = ex_min(ex, a);
        return ex_foo(ex, c, d, x, y);
    }
}

Translating from Scheme-like syntax (eliminating 'return') will
require automatic distinction between special forms and application
forms.

There's another thing that's really annoying: nested Scheme/lisp
expressions return values, but C's curly braces represent nested
statements.  Maybe it's best to simply target GCC's statement
expressions[1], and use the '?' condition operator.

This seems more natural.  I'm also using a direct compiler instead of
macros.  To make this look better though, the indentation needs to be
changed so statement expressions can be assigned to variables to
reduce the visual complexity of the nesting.

That's a pretty-printing exercise, so consider the real problem
solved.  The rest is play ;)

So, prettyprinting.  What you want is "newline" to indent to the
current indentation point.

--

Trying something else: rewrote it to make it more _manual_ :
essentially the code makes decisions locally about whether to "press
enter" or whether to change indentation or not.  This seems to work
quite well.

(emit-definition
 '(define (foo a b c)
    (let* ((x (plus b c))
           (y (let* ((d (min a b))
                     (e (max a b)))
                (times d e))))
      (bar x y))))
(pp-enter)

=>
_ ex_foo(ex *ex, _ a, _ b, _ c)
{
  return ({ _ x = ex_plus(ex, b, c)
            _ y = ({ _ d = ex_min(ex, a, b)
                     _ e = ex_max(ex, a, b)
                     ex_times(ex, d, e); })
            ex_bar(ex, x, y); });
}


OK..  I now see that this is not trivial because it requires
backtracking.  Better use something that's seen some thinking[3].

[1] http://gcc.gnu.org/onlinedocs/gcc/Statement-Exprs.html
[2] http://www.iai.uni-bonn.de/~ralf/hw2001/9.html
[3] http://planet.plt-scheme.org/display.ss?package=pprint.plt&owner=dherman
[4] http://eprints.kfupm.edu.sa/69518/1/69518.pdf

Entry: Terminology
Date: Thu Aug 20 22:59:14 CEST 2009

I shouldn't be calling the code graph an AST (abstract syntax _tree_)
since it is a _linked_ AST, which in general is a graph: variable
references linked to their definition points.

In PF all variables refer to code: primitives, quotations or SUB
cells.


Entry: PF interpreter in Ocaml
Date: Fri Aug 21 08:14:26 CEST 2009

I'd like to encode the interpreter data types in something more rigid,
to see if I missed some corner case.

The first thing to realize is to distinguish types and constructors.

      SEQ = (SUB : SUB)            ;; `:' is graph CONS
      SUB = PRIM | QUOTE | SEQ
 
      K   = HALT | (SUB . K)      ;; `.' is linear CONS

type sub  = 
    Prim
  | Quote
  | Seq   of sub * sub ;;

type k = 
    MT 
  | Frame of sub * k ;;

The second thing is to realize that dynamic typing is a _lot_ less
restrictive than the ML type system.  Especially the wrapping
necessary make unions of other types.  Dave Herman describes it
here[2] in relation to the way it's done in Typed Scheme[3].

type sub  = 
    Prim  of prim
  | Quote of datum
  | Seq   of sub * sub

and k = 
    MT 
  | Frame of sub * k

and datum =
    Number of int
  | Code of sub 
  | Pair of pair

and prim =
    Address of int

and pair =
    Nil
  | Cons of datum * datum

;;

Now I'm trying to express the interpreter.  Ocaml's syntax is weird.
Constructors have different syntax from functions?

Aha.  Ok I understand.  Constructors can take multiple inputs, but
functions always take a signle one.  Multiple inputs are handled using
currying.  A constructor seems to be a function that takes a tuple.

Next: how do you put a function in a datatype?  I.e. the prim type is
actually state -> state.  Looks like I want GADTs[4][5][6].  This
looks like a can of worms I don't want to get into yet.  Let's use
simple enumeration.

Good.  It's running.  See code here [7].

I've added 'Run' which leads to some arbitrariness in encoding the
types.  Is Run a prim or a special case of code?  I'd say the latter
since it operates on the whole machine state instead of just the
parameter stack.

Quite revealing to have to specify things a bit tighter..  From a
first real contact, I think I like OCaml, even if it is more
restrictive.  It is definitely more easy to get things right once they
pass the type checker.  I should have a closer look at Typed
Scheme[3].

Now, getting the 'fac' example running. In factor:

  : fac ( n -- n! ) dup 1 = [ 1 ] [ dup 1 - fac ] if * ;

What is necessary is a way to define a cyclic data structure.
Apparently `let rec' does not allow this:

let rec _fac =
  Seq(_dup,
  Seq(lit 1,
  Seq(_equals,
  Seq(quot (lit 1),  
  Seq(quot ((Seq(_dup,
             Seq(lit 1,
             Seq(_minus, 
                 _fac))))),
  Seq(_if, 
      _multiply))))));;

=> Error: This kind of expression is not allowed as right-hand side of `let rec'

How to solve this?  In Scheme it's easy: just dump a symbol at first,
then walk the data structure and set! the value.


[1] http://lambda-the-ultimate.org/node/983
[2] http://calculist.blogspot.com/2008/09/true-unions-revisited.html
[3] http://www.ccs.neu.edu/home/samth/typed-scheme/
[4] http://www.haskell.org/haskellwiki/Generalised_algebraic_datatype
[5] http://okmij.org/ftp/ML/GADT.ml
[6] http://www.haskell.org/haskellwiki/GADTs_for_dummies
[7] http://zwizwa.be/darcs/libprim/pf/pf.ml
[8] http://caml.inria.fr/pub/ml-archives/caml-list/2008/12/460de486fcbbc2eca13e5c77d014da17.en.html



Entry: PF interpreter in Haskell
Date: Fri Aug 21 17:02:45 CEST 2009

That was fun.  Let's try the Haskell version.  First, how does this
work..  Loading the file pf.hs in ghci:

> :l ~/libprim/misc/pf

To reload:

> :r


Oops.. ran out of steam..  maybe one lanuage in one day is enough..

I wonder though: the only apparent difference with Ocaml (apart from
the pure and lazy things) is that constructors seem to be curried
functions, while on Ocaml they take tuples.


[1] http://learnyouahaskell.com


Entry: Remarks about the PF interpreter
Date: Sat Aug 22 08:15:00 CEST 2009

Refer to entry://20090816-120957

Note it is important to distinguish the byte code graph data structure
from an AST or the untyped s-expression encoding of _symbolic_ code.

The SEQ type can have sharing (shared subroutines) or loops (direct or
mutual recursion), while an AST or an s-expression is really a flat
_tree_.


    - symbolic source code like:  ``1 dup cons''

      here the symbols are translated by a dictionary (vocabulary)
      that maps symbol -> SUB.  symbolic code is a _tree_ represented
      as an _untyped_ s-expression.

    - `byte code graph'.  Each symbolic source code definition can be
      compiled to a single SUB (subroutine) type, which is _directly
      linked_ to other SUB types by data sharing.  Because it's a
      graph, this data structure is not easily printed.

    - AST: because the original syntax is so simple, the intermediate
      form one would traditionally use in an interpreter = a data
      structure with a recursive type that represents the same _tree_
      as a source code s-expression, is _not_ present in the current
      PF implementation.  Compilation goes straight from symbolic code
      as non-annotated s-expressions to the `byte code graph'.


IMPLEMENTATION

      One could _implement_ a SEQ(foo,bar) as the following machine
      code:

                  call <address of foo>
                  jump <address of bar>

      Note that the `first' slot in a SEQ will push the return stack
      when it points to another SEQ.  If foo were a prim, the `call'
      instruction could be replaced by inlined machine code
      implementing the primitive, or a call to a machine code
      subroutine.

      In the case 'bar' is a subroutine that's used only once, it
      could be inlined to eliminate the jump.

      Alternatively, if the machine has a literal push instructiom, an
      even simpler representation would be something like:

                  push <address of bar>
                  jump <address of foo>



Entry: The return stack should contain XTs
Date: Sat Aug 22 10:40:14 CEST 2009

Something that has always bothered me about Forth is that the return
stack contains _pointers_ to XTs (i.e. XT _arrays) while `execute'
takes XTs.

In other word: the return stack _itself_ is not a program.  This is a
consequence of having two forms of code: threaded code and machine
code.  An XT is often implemented as a pointer to machine code (direct
threaded) or a pointer to a pointer to machine code (indirect
threaded).


Entry: PF closure
Date: Sat Aug 22 10:46:47 CEST 2009

Because there is no nested lexical scope, there are no closures as
there are in a functional programming language based on the lamda
calculus.

However, they can be simulated by `consing' data onto code.


Entry: Ocaml and tagged interpreters
Date: Sat Aug 22 12:23:04 CEST 2009

It seems to me that not being able to define data types that can
contain functions is quite a limitation.  Maybe there is something I
didn't get?  Maybe this only can't be done _recursively_ ?

I.e. what I want is:

type sub =
    Prim    of (state -> state)
  | Seq     of sub * sub (* NL *)

instead of having to break it down to tags where each state -> state
function is interpreted.

Actually.  Look at [1] Function types.  

Also [2] Recursive values which are not functions.  About recursive
values: apparently it is only able to handle `simple' expressions.
``Informally, the class of accepted definitions consists of those
definitions where the defined names occur only inside function bodies
or as argument to a data constructor.''[3]

[1] http://caml.inria.fr/pub/docs/oreilly-book/html/book-ora016.html#toc21
[2] http://caml.inria.fr/pub/docs/oreilly-book/html/book-ora016.html#toc23
[3] http://caml.inria.fr/pub/docs/manual-ocaml/manual021.html#s:letrecvalues

Entry: Next
Date: Sat Aug 22 15:22:33 CEST 2009

After the small excursion to Ocaml land (maybe I should forgo the
Haskell version?) it's time to get back on track and get this thing
working before next week.

Goals: 

 - tasks/assymetric coroutines with proper stack switching.

 - automatic wrapping of C APIs.



Entry: Closures / Different continuation types
Date: Sat Aug 22 18:22:21 CEST 2009

The 'cons' trick to make a runnable continuation out of lists
(possibly consed to existing code) doesn't work: code needs to be
quoted.

> '(1 2 3) run
> ERROR: unknown-instruction: 1

Maybe it was really a good idea to go for the Ocaml route: now I can
try to express these things in a type system to eliminate glossing
over those level-shifting tricks that tend to seep in to ``just make
it work''.  This is something to be _really_ careful about.  Something
I've only learned to appreciate recently..  It makes things more
complex, but it is well worth it when you try reasoning about the
code.  Guy Steele mentioned this in [1].  It is also quite central to
the PLT module system[2].

Intuitively: it is about a more general principle of not giving into
the ``turing machine feedback loop''.  Recursion (or iteration) is a
fine tool, but too much mixing makes things hard to understand.  More
often you want to abstract direct recursion into combinators.

For my taste Joy relies too much on `i' which constantly shifts
levels.  However it does illustrate a point: if you want a really
simple language: eval everything.

What would work is to have different 'frame' types to instruct the
interpreter to handle these speical cases.  Also, the stuff you pinch
off of a continuation stack should really be opaque.  Composable, yes,
but not decomposable..  Maybe there could be two kinds: interpreting
and non-interpreting frames?  So it looks like we need different kinds
of continuations after all..  They can all be differently tagged
linear pair types though.


[1] http://www.ai.mit.edu/projects/dynlangs/wizards-panels.html
[2] http://www.cs.utah.edu/plt/publications/macromod.pdf


Entry: Quote of the day
Date: Sat Aug 22 18:50:04 CEST 2009

Attributed to Matthew Flatt:
 ``You need a certain amount complexity to make things look simple.''

[1] http://list.cs.brown.edu/pipermail/plt-scheme/2009-August/035243.html



Entry: Asymmetric Coroutines in Lua
Date: Sat Aug 22 22:39:04 CEST 2009

It makes the point that, like partial continuations (PC), asymetric
coroutines (AC) are easier to use because they are composable just
like routines, while symmetric coroutines are hard to set up.

Coroutines cannot yield when there is a C function call in the stack.

The authors mentioned they were working on investigating the relation
between AC and PC.  From what I tried in Scheme, an AC can be
implemented in terms of a PC.  Also the other way around?  They both
are essentially segments of a control stack.

The update is here[3][4].

A library by Sandro Magi (naasking)[5][6].  Looks like this is the
place to contiue working.  I missed it last time, since I went to the
sigfpe[7] article for inspiration.  Currently, libprim/task.c has
stack copying coroutines.

So, what am I going to do?  Stack copying already works, so it could
be a fall-back.  Implementing one-shot continuations could then be
implemented as an additional feature.

[1] http://lambda-the-ultimate.org/node/438#comment
[2] http://www.inf.puc-rio.br/~roberto/docs/corosblp.pdf
[3] http://lambda-the-ultimate.org/node/2868
[4] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.4017
[5] http://code.google.com/p/libconcurrency/
[6] http://higherlogics.blogspot.com/2008/07/coroutines-in-c-redux.html
[7] http://homepage.mac.com/sigfpe/Computing/continuations.html
[8] http://www.reddit.com/comments/6s5tt/coroutines_in_c


Entry: Making partial continuations abstract
Date: Sun Aug 23 09:35:40 CEST 2009

So, it looks like it's clear that PCs are the proper abstraction
mechanism: they are k-stack segments that can be concatenated to the
current continuation.

There are several ways to approach this.  However, considering that
the tagging needs to use _linear_ memory, the appropriate way is to
tag the pairs.  If this is the way to do things, is it possible to
move other kinds of tagging to a ``snapped'' pair-based encoding?

I.e. instead of Seq(Quote(a), b) use SeqQuote(a, b)

It doesn't look like it: there are several disadvantages to not having
Quote(...) and Prim(...) as separate instances of a sub (subroutine)
type.  These are used a lot as a single type and should be bundled, so
they don't complicate matters.

So what about this:



and cont  = 
    Done  
    | Next of sub * cont
    | Late of datum * cont

Where 'Late' has the implied semantics of using `interpret' to convert
datum -> sub.  This would allow certain kinds of cheap, linear
reflective behaviour to be implemented.

( Note: I'm _only_ trying to work around the construction of a Quote
type, which is nonlinear.  I.e. it is a flattening optimization that
allows the use of linear data only. )

What about calling 'Late' something like 'Data' or 'Close'.  It really
doesn't need `interpret' semantics: it can be a pure quote, because
the call to `interpret' can be inserted directly into the
continuation.

This looks alright.  I just have to convince myself that this is
elegant: I.e. that it's really necessary to have 2 quotation
mechanisms: one part of nonlinear code (enabling data to behave as
code + be shared), and one part of linearly constructed code (data as
code, but not shared).  

Let's give it some fermentation..

EDIT: One thing to note is that the continuation doesn't need this
whole tail-recursive constraint: it is always consed to the empty
continuation, which leads to a machine halt state.  So it really _is_
a pair.

All this is because of subtle differences is between `code', `closure'
and `continuation'.  (I need a better name for `closure' though.)




Entry: Tree permutations
Date: Sun Aug 23 10:00:42 CEST 2009

It's suspiciously difficult because of the possible type exceptions
that might mess up temporary storage on the data stack.  So let's
forget about optimality and write well-factored code.

A linear language specification needs a sublanguage for specifying
_tree permutations_.  Essentially, once you've figured out which
pointers permute, the inverse is just the inverse permutation.  This
needs to be split in two parts:

      - make sure the tree has the structure you expect (type check)

      - perform permutation

Essentially, I want th tree transformation to be safe and guaranteed
linear in the first place, and correct in the 2nd place (which is then
easily checked in the terminal).

Probably Knuth has something to say about this.  Also, I ran into a
tree transformation language a while back.  Forgot where..  Was a
Belgian guy.

Also, I did something like this before.  Probably hidden somewhere in
Staapl / Brood code archives..  Poke?

This looks like a nice exercise to write a compiler for.

Can it be done using binary tree rotations?

( Really, I've done all this before..  But where? )

Suppose the stack is this: (L . S)

`cons' will be something like  (L . (d . S)) -> ((d . L) . S)
then the reverse `uncons' is:  ((d . L) . S) -> (L . (d . S))

They are indeed simple tree rotations.

A left rotation can be transformed into a right rotation as
LEFT = FLIP * RIGHT * FLIP

Nope!  They are not tree rotations.  They are rotations followd by a
particular flip:

               LEFT                 FLIP
(L . (d . S))  --->  ((L . d) . S)  --->   ((d . L) . S)

Anyways, it doesn't seem so difficult to compile such a definition
into a program that deconstructs the list, and then re-uses the list
cells to reconstruct it.

This can even be done manually.  I've summarized it in the next post.



Entry: Writing linear tree transformations
Date: Sun Aug 23 10:58:56 CEST 2009

/* Implementing (conservative) tree transformations. 

   A tree transformer can be derived using the following manual
   compilation approach:

       1. Write down the transform in a high level dotted pair form:
          LHS -> RHS
    
       2. Following the structure in LHS, bind the nodes (and
          deconstructed dot pairs) to C lexical variables using
          type-checking casts for the pairs.  This makes sure
          exceptions happen before we mutate anything.  

       3. Rebuild the tree by re-using the pairs to create the dotted
          tree in the RHS.

   To increase readability, index the pairs from left to right as they
   appear in the textual form of LHS and RHS.

*/


/*  D = datum
    L = list
    P = parameter stack
    
      (L . (D . P)) -> ((D . L) . P)
*/
void pf_cons(pf *pf) {
    pair *dot0 = CAST(lpair, pf->p);
    pair *dot1 = CAST(lpair, dot0->cdr);
    _ L = dot0->car;
    _ D = dot1->car;
    _ P = dot1->cdr;
    pf->p     = VEC(dot1);
    dot1->car = VEC(dot0);
    dot0->car = D;
    dot0->cdr = L;
    dot1->cdr = P;
}

/*  D = datum
    L = list
    P = parameter stack

      ((D . L) . P) -> (L . (D . P))
*/
void pf_uncons(pf *pf) {
    pair *dot0 = CAST(lpair, pf->p);
    pair *dot1 = CAST(lpair, dot0->car);
    _ D = dot1->car;
    _ L = dot1->cdr;
    _ P = dot0->cdr;
    pf->p     = VEC(dot0);
    dot0->cdr = VEC(dot1);
    dot0->car = L;
    dot1->car = D;
    dot1->cdr = P;
}



Entry: peg/leg leaky?
Date: Sun Aug 23 11:18:05 CEST 2009

Let's clean up the reader.  There is a problem with the peg/leg
parser: it uses malloc() but no free().  OK for one-shot tools but if
I'm going to keep using it, it's probably best to handle that
problem..

Ok, it apparently reuses its buffers, and grows them whenever
necessary.  I guess that's OK.

So, now I just need to be careful to not upset it.  Probably best to
use malloc()


Entry: Different continuation types
Date: Mon Aug 24 11:03:53 CEST 2009

Continuing the ideas in [1].  Starting with the following context:

  * A partial application is a binding of data context to code.
    Suppose you have an object referenced by `ob' and an operation
    `op' that expects such an object as the first parameter on the
    stack, the following code would be a closure:

          (ob op)

  * In Joy, code == data.  Its VM wouldn't need to distinguish pairs
    for code, partial applications and partial continuations.  Their
    difference is only visible through usage: they are all represented
    as lists.

  * In PF, these 3 need a different representation because code is
    nonlinear, while partial applications and continuations are
    linear.
    




What about this:

and stack = Empty | Next of datum * stack

and cont  = Done  | Next of sub   * cont
                  | Data of datum * cont

Wait.  There is already a mechanism to construct partial applications
as continuations: `dip' followed by `run'.

The question is then: what is best?  Explicit tagging (using different
continuation frame types) or ``prefix parsing''.

It seems to me that tagging is better behaved: in that case each frame
has its meaning isolated, and doesn't depend on the meaning of the
previous frame.  So be it: tagged frames.  This also makes `dip'
simpler.

[1] entry://20090823-093540



Entry: A Joy machine
Date: Mon Aug 24 12:21:55 CEST 2009

In [1] you find an intensional Joy machine with continuations
implemented as intensional quotations constructed at run time as the
machine performs name -> quotation dereference.

This is an example of how a reflective feedback loop (turning data
into behaviour) can make a language interpreter really simple.

The flip side is that reflection breaks static analysis.

[1] http://zwizwa.be/darcs/libprim/pf/joy.ml



Entry: SEQ -- why encode the byte code as a binary tree instead of nested lists?
Date: Mon Aug 24 17:16:15 CEST 2009

(From a discussion with Dominikus.)

Why use a binary tree instead of a list to represent byte code?

We start with the assumption that we agree on representing byte code
as a recursive data type (i.e. as opposed to using low-level arrays
and pointers/indices), and that we agree on wanting proper tail call
handling so loops can be expressed as recursive calls without blowing
up the context stack.

To do this correctly, given a concatenative function specified by the
code sequence "a b c ... z", the _last_ procedure `z' needs to be
handled in a special way.  Before `z' is invoked, no context needs to
be saved because there is nothing to do afterwards.  This is in
contrast with the function calls `a', `b', `c' ... which need the
computation to resume and thus require a context save.

Now, in the case of a compiled language, you have two options.  You
either perform this special treatment of the last element in the list
at run time, or you perform it at compile time and represent the code
in a data structure that doesn't need any special case handling by the
interpreter. 

This is exactly what SEQ does: it provides a data structure which can
be interpreted without special-casing.  The `handling' of tail calls
is done at compile time only, by encoding the program as a binary
tree.  Interpreting the binary tree has only one case: the first
branch needs context save, while the latter one doesn't.


Entry: Rant about reflection
Date: Mon Aug 24 19:04:41 CEST 2009

PF isn't really so much about Forth.  The name is a historical
accident..  PF is really about building a simple, safe, dynamically
typed language with linear memory management.

Let me rant a bit.

Forth is.. well.. Forth :) There isn't much to say about it, except
that it's a very nice machine model with a hacked-together text
interpreter that's really too reflective.

Over the last couple of years I've moved more into the direction of
less reflective code, mostly guided by the PLT Scheme approach to
domain specific languages.  The attempt in Staapl to ``unroll'' the
very reflective nature of standard Forth stands as one of the pilars
of my approach to concatenative languages.

In general the pattern seems to be that using very reflective
languages leads to very simple language cores and a lot of expressive
freedom, but in turn makes it easier to write programs that are harder
to understand.

In a static programming style you put the cleverness in the types and
other static constraints: something a compiler can check and proove
consistent.  In a dynamic language the cleverness goes right into the
core of things: anything can be special-cased.

The main point being that when code bases grow it seems to be
beneficial to be able to bolt things down in a more static way: huge
code bases can't be kept in a human head so you need a machine to help
manage at least part of the consistency.  Adding some static
properties seems to be the only way to do that effectively.

I'm currently right in the middle of these two paradigms: I realize
dynamic languages are essential for debugging and exploration, but
also that static structure is necessary to put more meaning into code
you write for "later" when it's meaning is no longer in your
short-term memory.  The holy grail seems to be to find ways to combine
these two in a productive manner.  Gradual typing seems to be the way
to go.


Entry: JOY: Intensional quotation
Date: Mon Aug 24 20:32:40 CEST 2009


Both intensional and extensional quotations can be _constructucted_ by
building larger objects that contain other quotations as subparts, but
only intensional quotations can be _deconstructed_ i.e. interpreted as
a list.

Because the use of extensional quotation requires two different types
with different operators, Manfred concludes[1] that ``On the whole,
then, it does seem preferable to have quotation as an intensional
constructor.''

PF (and Scat in Staapl) do it differently to be able to compile
quotations to a more efficient form.  MetaOcaml does it differently to
not loose well-typed guarantees.  In general it seems to me that
extensional quotation is the sensible default: loose reflective object
language expressivity to gain meta language expressivity.



[1] http://www.latrobe.edu.au/philosophy/phimvt/joy/j07rrs.html
[2] http://en.wikipedia.org/wiki/Intensional_definition
[3] http://ccl.clozure.com/irc-logs/scheme/2008-10/scheme-2008.10.22.txt


Entry: rewriting -> Factor list
Date: Tue Aug 25 08:00:05 CEST 2009

Hello list,

I guess this is mostly for Daniel, but anyone else of course feel free 
to chime in...

I'm trying to get a better idea about rewriting systems for
concatenative languages, particularly in the area of specification of
peephole optimizations.  It's been on my mind for a while, but I've
always side-stepped any non-deterministic computations.  I.e. in
Staapl[1], the PIC18 code generator[2] is written using an eager
pattern matching approach, essentiall re-interpreting already
generated machine code as stack machine code that can then be
partially evaluated together with the currently compiling primitive
operation.  (Explained here[3] in some detail).

I've always been suspicious that this formulation, while it works well
in practice and has a certain elegance, is a special case of a more
general, non-confluent rewriting system.  

It seems to me that if there are any interesting optimizations to
make, they are not going to be unique and depend on which path through
the reduction rules is used to get to a particular irreducible
expression.

And then it stops.

I have some intuition about reduction machines, as long as they
compute a unique solution (lambda calculus w. strict eval (CEK
machine) and lazy eval using graph reduction, ...) but I'm not sure
how to view the other cases where you have a bunch of rewrite rules
and you ask the compiler to pick a sequence of reduction rule
applications that leads to the ``most optimal'' reduction under some
kind of cost function (for PIC18 this would be the machine code
length).

I hope this makes some sense..
Any ideas on this welcome.

Cheers,
Tom

[1] http://zwizwa.be/staapl
[2] http://zwizwa.be/darcs/staapl/staapl/pic18/pic18-unit.ss
[3] http://zwizwa.be/archive/staapl.html


Entry: Real-time GC
Date: Tue Aug 25 08:34:16 CEST 2009

PF uses linear core memory management and nonlinear code memory
management.

With linear memory management I mean that the machine's run-time uses
_only_ a tree data structure.  SEQ encodes all the rest, and is the
basic data structure of the nonlinear memory.  (Essentially, SEQ is
only constructed at compile time: during run-time the code structure
is constant.)

The reason for this is that (in my understanding), "real" real-time 
tracing garbage collection doesn't exist.  It exists for memory-only GC, 
but in practice you also want to manage much more scarce hardware
resources which in the light of a real-time GC _still_ require full
non-bounded collection steps.

If you replace a tracing GC with refcount management system, then you
can have real-time GC, given that you realize GC cost is proportional
to the _size_ of data structures you destroy.  In practice, this isn't
a problem.  An embedded system usually has a loop with very
predictable data usage.  The case where you destroy a single data
structure containing a large amount of objects is rare.


Entry: Metacircular interpretation: Maxwell's equiations of software
Date: Tue Aug 25 10:27:08 CEST 2009

[1] http://arcfn.com/2008/07/maxwells-equations-of-software-examined.html


Entry: Next
Date: Tue Aug 25 10:37:26 CEST 2009

Implement the 2-type continuation frames in C.  This requires two
kinds of linear cells: data cells (list cells) and contiuation
k-cells.

Got it..  let's see if `dip' works.

OK.

shift/reset now also work again.

The implementation (using linear pairs for data and `next' frames for
sequence continuations) is a bit messy.  Maybe there is something to
say about doing it like this..  Too reflective?  Let's keep it like it
is and see if there are any inconsistencies.

Summary: it's now possible to `cons' a datum onto a partial
continuation, which is implemented as a nested data/next frames, where
the data frames are just the linear list object.

This is still not correct:

Essentially one wants a `bind' and a `compose' operation which
combines a datum or a function with a function.



Entry: What is a continuation?
Date: Tue Aug 25 17:33:50 CEST 2009

That's the down-to-earth bit-twiddler's ``is''..

A continuation is essentially (a representation of) an imperative
program that sequences the remaining primitive machine operations to
perform to complete the current program/evaluation.  The side effect
of this imperative program is the evaluation of the expression it
represents.

Obvious if you look at what CPS-conversion does: it translate a
functional program written in terms of nested expressions, into an
imperative program that incrementally builds and updates a data
structure.

Now, this becomes really obvious in a concatenative (point-free)
language, which already is in CPS form.  The continuation is
essentially a program in the same form as any other program.


Entry: PLT Redex
Date: Tue Aug 25 17:46:05 CEST 2009

Jao calls it syntax-rules on steroids[1].  Looks like the answer to
the question about the peephole optimizer's rewriting behaviour has
been right under my nose for quite a while.

[1] http://programming-musings.org/2009/06/19/a-merry-gang/
[2] http://redex.plt-scheme.org/


Entry: Re: rewriting
Date: Tue Aug 25 21:31:25 CEST 2009

Thread + reply from Daniel:
http://www.mail-archive.com/factor-talk@lists.sourceforge.net/msg03559.html

> If the rewrite rules are monotonic, then there's an easy procedure
> to optimize the program: list out all programs, calculate their
> efficiency, and select one of the best ones

I guess here `monitonic' means that there are no loops?  Something
like `strictly decreasing'.

> With something like an optimizer, I don't understand why things have
> to be confluent. 

This corresponds to my intuition also.

> Maybe the algorithm to find the optimal series of peephole
> optimizations will be a confluent subset of the general rewriting
> system of all valid peephole optimizations; is this what you mean?

What I meant was that the algorithm in Staapl is formulated
differently (each word is a macro: a _fuction_ which takes an object
code stack to an object code stack).  I don't immediately see how this
can be reformulated as a more general rewrite system on code _syntax_,
but I've got this hunch it's not so difficult.

I'm also not sure if they are really 100% equivalent, since I'm
throwing in some extra specification of the order of evaluation.  This
makes it possible to play tricks like using objects that only exist at
compile time without requiring any special notation for it: the eager
algorithm guarantees that such objects are fully reduced before code
is compiled (or raise an error).  What would be neat is to be able to
keep this semantics, but use a more elegant way of formulating it as a
syntactic operation.  (If this doesn't make sense I'd be happy to
elaborate.)

> On register-based architectures (like the PIC18, right?), doing
> everything with a stack is inherently inefficient because of peeks
> and replaces on the top of the stack.

The 8-bit PIC chips are weird.  The 16-bit cores are more or less
standard register/risc but on the 8-bitters everything needs to pass
through a single working register.  This makes it almost cry for a
stack language approach :)

> I guess you do the equivalent of DCN by your peephole
> optimizations. 

DCN = deconcatenation?

> The thing about Factor's compiler is, it does this in a more general
> way. You should look at Slava's past blog posts, or read the code,
> if you're interested in knowing more about it.

Bout time I have a look at it then..  Is the compiler tied in closely
to the rest of the system?  I'm wondering if I it would be difficult
to embed it in PLT Scheme through the concatenative -> Scheme wrapper
macros used in Staapl.

So, your basic idea seems to be that yes, you want something that's
not confluent, and no, there is no general algorithm: general
rewriting is too unstructured, so a special purpose approach is
required.

Thanks so far for these answers!

Cheers,
Tom





Entry: Abstract continuations
Date: Wed Aug 26 11:12:17 CEST 2009

Time to byte the bullet and make the code more abstract.
TODO: make the polymorphy more highlevel on next change.



Entry: Partial Application
Date: Wed Aug 26 11:55:02 CEST 2009

The pf.ml machine viewed as a non-linear machine can have a `sub' for
continuation.  To do this linearly one needs:

      1. a way to wrap up a a linear datum as something runnable

      2. allow for the composition of runnable things.

It turns out that `compose' is a much better base to build on than
partial application, which is a special case of run-time data -> code
conversion and runtime code composition.

In the current form it needs an extra wrapper for nonlinear -> linear
data, but after that, `compose' is just list concatenation where the
CONS cells are continuation frames of `dip' and code sequencing.



Entry: PF: Linear code composition with `compose'
Date: Wed Aug 26 15:47:04 CEST 2009

The `>lcode' word _projects_ linear code and nonlinear code to linear
code.

The `abstract' word _wraps_ any atom as linear code.

The `lcompose' word takes two linear code words and composes them.

The `compose' word takes two elements on the stack, projects them to
linear code using `>lcode', and composes them with `lcompose'.

The `pa' word takes two elements on the stack, converts the first one
using `abstract' and the second one using `>lcode', and composes the
resulting linear code with `lcompose'.

A partial continuation obtained using `shift' and `reset' is
represented as linear code.  A full continuation obtained from
`call/cc' is currently implemented as a partial continuation that
ignores continuation marks inserted by `reset'.  The `run' word and
with it all combinators take both linear and nonlinear code.

They do _not_ take data however.  In that light I'm still not
convinced that `>lcode' should automatically convert data to code.
There is probably need for an `>ldata' word too.


Summary:

        Anything that represents code can be passed to `compose' to
        yield a code quotation that can be passed to `run' and higher
        order combinators.




Entry: Shift / reset
Date: Wed Aug 26 23:19:22 CEST 2009

In Scheme, `shift' binds the partial continuation to a value and
then passes this to a body, shifting it in place of the removed
frames.  Looks like `reset' and `shift' need to be exchanged!

More tomorrow..

(shift 1 2 3) reset
> > ps
<1> <,{'1 '2 '3}>
> run
> ps
<3> 1 2 3
> 

It is `reset' that marks the continuation stack and `shift' that packs
up the continuation.  So it looks like it is correct.  The only
strange thing is that in a CPS language, the ``body of shift'' will
come after `reset' !


Entry: Concatenative languages and Full continuations
Date: Thu Aug 27 14:32:29 CEST 2009

From a discussion with Dominikus I come to conclude that it's
important to distinguish different kinds of continuations:

  * Full state continuations, that capture the entire machine state:
    both parameter and context stack.

  * Context-only full/partial continuations, that capture marked
    segments of the control stack and view the whole data stack as the
    value to be passed.  

Note that marking the data stack is ill-defined, as it may be popped
``beyond the mark'' before it is captured.

Can both be unified?

Rephrasing one of the solutions Dominikus proposed, one could wrap the
stack as a quotation, and concatenate it onto the remainder of the
current program being evaluated, yielding a new program that
encapsulates the full continuation.

Let's implement this in PF and see where it takes us.


  
OK.  After doing this it seems that

 * You still want partial continuations in this context: in most cases
   you're not interested in the toplevel drivers, just in the control
   that's part of the program.

 * The operation factors out into two steps: ordinary continuation
   operators that ignore the parameter stack, and a separate operation
   that appends the stack as a quotation to the resulting
   continuation.


Thinking a bit further and keeping an eye on how it's implemented in
Scheme, it seems that this generalizes to any kind of threaded state
mechanism (i.e. FP-style matrices, Monads, Arrows, ...?)




Entry: simpler primitives
Date: Thu Aug 27 15:03:09 CEST 2009

As mentioned before: there are too many things to think about
implementing linear primitives (possible GC restart and mutation).
This reduces my confidence that the code is correct in the presence of
primitive error aborts or GC aborts.

So, either I give up on the idea of GC restarts, or I factor the code
such that these distinctions are no longer a problem.



Entry: TODO
Date: Fri Aug 28 11:26:56 CEST 2009


- define partial continuations better

- check out the multipe state threading in dominikus' code

Entry: Partial Continuations in a Concatenative Language
Date: Fri Aug 28 13:19:08 CEST 2009


Look here[1] for links of the theoretical background.

First, the current implementation of `reset' and `shift' are _wrong_
-- actually doubly wrong:

* the _captured_ by `shift' is delimited.
* the _shifted expression_ is placed in a delimited context.

I'm using an interface that doesn't even have a shifted expression.

So let's fix this: the API is now:

   shift / control ( quot -- )

Where the quotation is called with the continuation on top of stack.





[1] entry://../compsci/20090828-144955
[2] http://www.bluishcoder.co.nz/2006/03/factor-partial-continuation-updates.html
[3] http://docs.factorcode.org/content/article-partial-continuations.html


Entry: GC restarts: clear separation between PURE and LINEAR functions.
Date: Fri Aug 28 21:01:50 CEST 2009

To catch GC restarts in the definition of linera primitives, a simple
trick can be used for testing: simply kill the reference to the GC
right before a mutation is made.

So, primitives are either purely functional and can have allocation,
or are linear and cannot perform allocation but are allowed to perform
in-place operations.

Let's say that a primitive starts in pure mode, but whenver it mutates
something, it switches to impure mode.

In order to test all primitives, a freelist = NIL + GC step can be
performed before the execution of each primitive to cover the case
where a primitive needs extra storage.


Practically:

- The VM: replace all PUSH/DROP operations with a FROM_TO. Start with
  making this type-tagged.  There is a real problem in the exception
  handler: it might fail as there is no guarantee there's enough
  memory.

- Find a more highlevel way of doing this.  Preferably static.  Can't
  this be solved as a type issue?  In C?

- Am I solving a real problem, or is this just a consequence of the
  way the GC is implemented?  The real problem is to make a
  distinction between PURE and LINEAR, where the latter cannot perform
  any allocations, and the former cannot perform any mutations.

- Try it first in Scheme.


Ok.  In Scheme it doesn't seem to be such a problem.

Now, in PF, can't we make LINEAR the default?  The problem is then:
how to specify nonlinear words?

Let's say it's allowed to switch to PURE mode at the entry point of a
primitive.

There's something fishy going on with exceptions in Scheme too:
exceptions should NOT perform allocations.  The _sc_step() might have
switched to linear mode, so the sc_make_error might trigger GC.

    switch(exception = setjmp(sc->m.r.step)) {
        case EXCEPT_TRY:
            PURE();
            rv = _sc_step(sc, state);
            break;
        case EXCEPT_ABORT: 
            rv = sc_make_error(sc, sc->m.error_tag, sc->m.error_arg, 
                               state, const_to_object(sc->m.r.prim));
            sc->m.error_arg = NIL;
            sc->m.error_tag = NIL;
            break;
        default:
            break;
    }

(fixed: sc_make_error is now called before _sc_step, to preallocate
such that the error allocation itself can't trigger GC restart).

Maybe primitive exceptions should be linear?  There are a lot of
nuances here..  It needs to be simplified.

EDIT: I'm still using the earlier paradigm of ``pure before linear''.
In the C code this requires respecting some invariants in an
error-prone dentralized way, but the principle itself holds and seems
to have some higher significance: 

   You can use pure funcitional operations as long as you don't change
   the world state.  I.e. it's either PURE or LINEAR.  And you're
   allowed to switch from pure to linear, but not the other way
   around.

Practically, this makes memory management easier in the presence of
brutally low-level C-code.  Also, it's quite simple to enforce
linearity: simply do not pass a reference to the allocator!  It's more
difficult to enforce purity however.  In C this can't be enforced (or
maybe it can.. using some `const' declarations..)


Entry: Primitive exceptions should be linear
Date: Sat Aug 29 09:39:12 CEST 2009

The main reason for this is that you really want the C model to be
independent of the GC.  Exceptions extend into C.

The _real_ problem is that in PF, there are occasions where you want
to perform a linear allocation, but you're not sure this is going to
succeed.  What about calling this a system failure?  I.e. always make
sure there are enough free cells.  In an embedded system it most
definitely is an unrecoverable failure.

The problem is that in current ``prototyping PF'' the linear cells are
allocated lazily.  Maybe a good solution would be to keep an extra
free buffer with a couple of cells N to guarantee at least N
allocations to succeed in the LINEAR regime where the GC is switched
off.  Then when the machine can take a GC restart, replenish this
buffer.

So, let's leave it as it is.  Fix the problem when there are
unexpected allocations in LINEAR mode.  Currently this seems to only
happen when errors are already quite severe.

Then, the only remaining problem is to make sure no _inconsistencies_
are introduced due to primitive errors by keeping linear data linked
to the VM instances at all times.



Entry: Fully linear PF machine
Date: Sat Aug 29 11:11:18 CEST 2009

Hmm... Maybe the trick is really to have a single linear <-> nonlinear
conversion primitive, and never perform any conversions in other
primitives.

Another problem: in _px_run() the `ip' can't be a C variable for the
same reason that other structures can't be present only on the C
stack: primitives might abort in which case the current `ip' is lost.

What about turning this around: 

  1. PF primitives can never fail due to GC restarts as they _always_
     run in LINEAR mode.  They will abort on an empty freelist, but this
     case can be handled separately and be made more gentle in the
     prototyping case (where you want to give up real-time behaviour
     for extra allocation).

  2. Anything that's nonlinear needs to be guarded from GC restarts in
     a different way.



Entry: Linear `compile'
Date: Sat Aug 29 14:15:06 CEST 2009

Now, because `compile' can't introduce any non-linearities, it could
be made to produce a linear code representation.  Only when code
enters the _dictionary_ need it become non-linear: since then it has
become random-access and there are no more guarantees about how it is
referenced.

Let's split this up in `compile-datum' and `map'.


Entry: Combining CDR-coding with vectors
Date: Sat Aug 29 19:24:12 CEST 2009

Instead of using vectors with tags in the name field, it might be
simpler to optimize the representation for the encoding of CDR-linked
lists.

This is to be able to give in to some performance issues:  speed is
not important for PF, but memory size might be.  Currently, CONS cells
are really expensive, requiring 3 pointer slots per cell.

However, since the rest of the implementation relies heavily on type
tags, maybe this isn't going to work well?  It might be better
reserved for a tagless representation..

Something for later.

[1] http://en.wikipedia.org/wiki/CDR_coding



Entry: PF: clear staging
Date: Sun Aug 30 11:09:35 CEST 2009

Everything in the code points to and optimal organization where linear
/ inplace programming and pure functional programming are clearly
separated.  Keeping both in the same VM might be useful for sharing a
memory model, and is _very_ useful as a user interface, but it
complicates matters.

The lessen to learn: this _is_ a staged approach, so it can be
implemented in a way where the object language / machine is completely
independent from all reflection, and where code and other
graph-structured objects are constant.

The good news is that this is a very good fit for Staapl.

What needs to be done is to find a way to express this in a typed
setting.  It is my hunch that the PF VM _itself_ might be better
expressed in a typed language, where linearity constraints can be
managed statically.

Essentially, a linear concatenative language is a _notation_ that
allows one to forgo linearity annotations, which seem to be necessary
in a applicative language with unstructured variable references.


Entry: A fully linear concatenative VM needs interpretation
Date: Sun Aug 30 11:50:31 CEST 2009

What I find really remarkable is that it is possible to make a pure
functional concatenative language fully linear.  You don't need _any_
graph structure, including its virtual machine!

However.  Full linearity of the machine requires some form of
interpretation (laziness).  When code is executed, name references
need to be expanded to their definitions.

This `lookup' cannot be performed in a separate compilation stage
(i.e. to eliminate its associated run-time cost) while maintaining
linearity of the VM.  Such a stage would directly link variable
references with variable definitions and require a non-linear (graph)
data structure:

   * Names can be used multiple times (i.e. procedure re-use) which
     requires a directed acyclic graph.

   * Name-based recursion can introduce loops in the representation.

In this light, the idea behind the PF machine is that you operate in
two regimes: one which remains linear, but keeps code and VM as
opaque, constant structures, and one which is nonlinear to implement
issues related to the VM (in the simplest case just code compilation).

The main idea is that: ``Names are part of the meta-language''[1].

[1] entry://../staapl-blog/20080813-125604



Entry: Using C's `const' to distinguish between PURE and LINEAR
Date: Sun Aug 30 17:13:21 CEST 2009

LINEAR is easy to enforce dynamically: simply don't provide a
reference to the allocator.  Pure is more difficult, but might be done
statically using C's `const' for return values of constructors.

Entry: Factor-like syntax
Date: Sun Aug 30 18:27:22 CEST 2009

Removed gratuituous incompatibilities with Factor: the parser now
accepts '{' and '}' for based list literals, and the pretty-printer
uses '[' and ']' to display decompiled code.

FIXME: this is broken: {{123}} -> '('(123))



Entry: Using Partial Continuations
Date: Mon Aug 31 09:34:24 CEST 2009

Let's try the classical inversion of control problem: convert a
traversal function into a cursor.

First example: convert a list into a stream by inverting `each'.

    (list->stream . ((((cons) shift) each '()) reset))

A `stream' is a pair of a value and a quotation or '().  


> '(1 2 3) list->stream
> > ps
<1> {1 . <,[each-next] '{2 3} '[[cons] shift] ,['()] ,[prompt-tag]>}
> uncons
> ps
<2> 1 <,[each-next] '{2 3} '[[cons] shift] ,['()] ,[prompt-tag]>
> run ps
> <2> 1 {2 . <,[each-next] '{3} '[[cons] shift] ,['()] ,[prompt-tag]>}
> uncons run ps
> > <3> 1 2 {3 . <,[each-next] '() '[[cons] shift] ,['()] ,[prompt-tag]>}
> uncons run ps
> > <4> 1 2 3 ()


This also makes it pretty clear that a partial continuation shouldn't
take the data stack, otherwise `list->stream' wouldn't work
correctly.  I.e. in the following you really want everything
underneath the literal list to be left intact.

> 123 123 123 '(1 2 3) list->stream
> > > > > ps
<4> 123 123 123 {1 . <,[each-next] '{2 3} '[[cons] shift] ,['()] ,[prompt-tag]>}



Entry: Onward
Date: Sun Sep 20 14:59:20 CEST 2009

Looks like the overall design is done.  In the mean time Dave Herman
fixed the show-stopping bug in c.plt, and I've started integrating it
in Staapl.  The plan is now to gradually move the libprim
implementation in C to a higher level description, so it can be
compiled to C or another substrate (i.e. a stack machine).  This would
also enable the linearity constraints to be machine-checked.

EDIT: the goal here should be to keep the current functionality
working at all times, and migrate it to a different implementation.


Entry: Summary
Date: Tue Oct  6 17:18:15 CEST 2009

What do we have now:

  * EX: a very simple GC supports an expression evaluation language
    that is used to implement the Scheme and PF machines.

  * Scheme: a very simple reflective Scheme interpreter.

  * PF: a VM for a linear memory concatenative language.

  * Some scaffolding code to generate wrappers.


What do we want:

  * Wrapper generation for a library of leaf objects.

  * Fully functional PF language.

  * Simpler way to implement PF primitives.


Entry: Images
Date: Tue Oct  6 17:25:41 CEST 2009

Goal: get the video in, XV out working in PF.

Strategy: use the old PF code, after factoring it into plain C OO
code.  Currently, the code in X11 seems already factored so should be
usable as-is.

Next: create a wrapper generator.  For scheme, it's usually simplest
to keep the parameter layout.  For PF however, object references are
usually best placed on top-of-stack, with all the other arguments in
reverse order.  

Something like this:    

          message(object, arg1, arg2) ->  arg1 arg2 object message



Let's try Scheme first, then use that as a base to write the RPN wrappers.

Step one: parse the file with c.plt
Took cplt.ss from staapl (includes simple printer).

Hmm..  This isn't going very well.  Maybe the hint I should take from
this is that my header files aren't abstract enough: they depend on a
lot of typedefs from deep within the system.  The C objects should
really abstract most if not all of this..

So, what's next?  Setting a clean header standard?  I guess this is
doable: allow for public and private versions, with public ones
exposing only functions.  Alternatively, working around the
limitations of c.plt might be simpler.

Filed bug report.

Note: there is a syntax for creating ASTs:
http://planet.plt-scheme.org/package-source/dherman/c.plt/3/2/planet-docs/c/pc.html


Entry: C parsing
Date: Wed Oct  7 09:09:29 CEST 2009

Ok, let's make it work with modified .h files for now.  No, the bug is
too much of a block.  The simplified headers won't parse either.  I'm
letting this go until the block is fixed.


Entry: PLT Scheme
Date: Wed Oct 14 09:28:21 CEST 2009

The current design of a combined Scheme / PF machine seems useful.
However, if there is no constraint on the _size_ of the scripting
system, it might be wiser to bolt a PF-like linear machine onto PLT
Scheme, in a way that is usable with other Scheme implementations.

Making the definition of PF more formal might be done in this way too:
providing a substrate in PLT Scheme which can compile to C and the
minimalistic Scheme/GC in librim.


Entry: Gstreamer
Date: Thu Oct 22 12:46:55 CEST 2009

Recent events have made me think it's probably best to bridge PF and
gstreamer.  However, it might be interesting to do this as a 3-way
coupling between plt-scheme, PF vm and the gstreamer framework.


Entry: Next: usability + test case
Date: Tue Nov  3 10:28:39 CET 2009

Need to evolve the design towards an (undisclosed) application.
Things to fix:

  * automatic wrapper generation

This might be the most interesting thing to start at.

  * unify the Scheme and PF cores into a single VM.


Entry: FFmpeg
Date: Tue Nov  3 10:36:12 CET 2009

Need FFmpeg encoder bridge + image format.

image:   256 level greyscale.
encoder: mpeg4, no audio

How to generate the glue code?  -> manually for now
(probably need both manual and generated glue code)

  Q: Is it worth it to use the C api?  Is that amount of control
     necessary?  Maybe a pipe bridge is easier?

  Q: What documentation to use?  Its difficult to find a good
     introduction for the C encoder api.

This seems to be the right spot[1].  Start with the file
libavcodec/apiexample.c[2].

The example file uses the following objects:

    AVCodec *codec;     // codec object
    AVCodecContext *c;  // configuration data for codec
    AVFrame *picture;

General structure is simple: picture data is converted to raw packet
based on current codec context.

Next: representing encoded data blobs is maybe best done using the
native string objects.

Problem: how to bridge wrapped buffers and C-level buffers?  I.e. the
low-level C objects need to be able to interface with leaf/bytes.h. 

The simplest solution seems to be to never let the C code allocate
anything (except for data embedded in locally defined objects).  

[1] http://ffmpeg.org/developer.html
[2] http://www.irisa.fr/texmex/people/dufouil/ffmpegdoxy/apiexample_8c.html


Entry: building a custom Scheme interpreter
Date: Tue Nov  3 16:16:06 CET 2009

How to build an interpreter with extensions?

Currently leaf objects are initialized in sc/scheme.c : _sc_new()

    /* Atom classes. */
    sc->m.p = malloc(sizeof(*(sc->m.p)));
    TYPES->ck_type = ck_class_new();
    TYPES->symbol_type = symbol_class_new(1000);
    TYPES->prim_type = (void*)0xF001; // dummy class
    TYPES->port_type = port_class_new();

In ex/ex.h : base_types (the classes) are accessed by the core
implementation through the sc->m.p member.

The problem that needs to be solved: a scheme factory method
`make-foo' needs to access the foo class object through the sc* scheme
VM pointer.

Question: do these classes really need to be tied to the VM?  The
problem is this: they need to be stored somewhere.  It's possible to
store them as a global variable : some C libraries do this anyway.

Essentially: 
 - do different VMs _need_ different class instances?
 - do we want to _allow_ this?
 - classes could be shared

Two conflicting requirements: it's better not to use any global
process data, but some form of sharing will be present when libraries
are used that do this.  Let's build the FFmpeg wrapper with global
classes.


Solution: _sc_init() takes a parameter = a struct containing classes
that will be stored in the sc->m.p member.  These can then be used by
procedures to access vm-global resources, where the implementation can
then decide to make them image-global (i.e. FFmpeg's init)



Entry: installed libprim scheme
Date: Tue Nov  3 17:26:21 CET 2009

Todo: 

 - make the header files compatible with system-includes.  I.e. use a
   single base (ex/... sc/...)

 - bundle libraries:

      libprim_ex.a
      libprim_sc.a
      libprim_media.a


Libraries are not a problem.  Header files are, because of
generation..  Separating in private / public will be a mess.

Ok, fixed it


Entry: Automatic wrapper generation
Date: Tue Nov  3 18:44:40 CET 2009

The idea is now the following: from a .h file containing a simple C
wrapper written in terms of known C objects, generate all the glue
code to register the functions with the VM.

For SC and PF, this amounts to packing/unpacking strings, ints,
... depending on the memory model used.

Essentially: the wrapper binds the pure C objects to a PF/SC memory +
control (exception) model.

The task should be simple: examine the .c / .h code and generate
anything that looks like a pattern.


Entry: The big plan
Date: Tue Nov  3 21:33:02 CET 2009

So, what am I going to do with this?

To be able to use it in commercial projects, I'm probably going to
switch the licence to LGPL or BSD.  The idea is that a simple .c
tarball (without dependency on mzscheme) can be used to build the .a
libs which can then be linked into an app.  DONE

The problem is that I'm still not positive about how to tackle the
wrapper generation.  This essentially bridges the C data model to the
EX data model, possibly wrapped differently for SC/PF.  This is still
too complex.  The architecture needs to be formalized.

The C function model should be something like dataflow functions in
Oz.  The idea being that the C code doesn't perform any data
allocation, but that function output is handled by in-place updating
the collection of basic objects.

I.e. SSA, but on an object-level.



Entry: The media lib
Date: Wed Nov  4 06:38:18 CET 2009

Collection of .c/.h files.
media.c : wrappers for scripting languages.

To generate the wrapper code it would be best to make sure that the .h
files are "clean" : no system libs, no delegate library libs.

This can be moved out of the critical path though.  Keeping the
general idea in mind, the ex functions can be coded manually.

The problem with class registration remains though.  Classes are
accessed as part of a struct, so they cannot be dynamic.


Entry: More formal architecture
Date: Wed Nov  4 07:19:19 CET 2009

C - side: objects, constructors, destructors and in-place functions.

The C functions can be bridged to EX function by wrapping C objects in
a GC managed struct.  This should be automated.

How can this automation be made as simple as possible?


Entry: Swig
Date: Wed Nov  4 07:28:54 CET 2009

Let's have a closer look at Swig[1].  It might be wise to make all
interfaces compatible with standard scripting languages through Swig.

The problem with Swig is that it generates large code.  It can really
be done a lot simpler..

[1] http://www.swig.org/Doc1.1/HTML/Contents.html


Entry: Are classes/types global?
Date: Wed Nov  4 07:49:31 CET 2009

I guess the answer is yes.  C objects contain pointers to C classes,
but the classes themselves do not really need to be tied to the VM
instance.

The real question is: what is a VM instance?

Let's reformulate: each VM has its own memory manager (GC).  This
means that managed data cannot be exchanged between VMs.

Really, a VM is kind of a global variable.

Why is there a need to reference the base_types struct?  Let's look at
the macros that use it:

#define DEF_ATOM(name)                                                \
    static inline name *object_to_##name(object ob, ex *m) {          \
        return (name*)object_struct(ob,m->p->name##_type); }

This essentially maps `name' to a pointer to its associated class/type
object for use in the `object_struct' function, which is the main
function that determines if an object matches a type, and returns a
pointer or NULL.

There is no real reason why m->p->name_FOOTYPE shouldn't be
globalvar_FOOTYPE, apart from global variable references being more
expensive.

So..  How to make the scheme interpreter extensible?  Can these core
implementation issues be hidden from user extensions?  The whole point
is to be able to simply wrap some C/C++ code..

Let's stick with:

  - interface: class_new() methods in the C code, to allow for most
    general approach.

  - policy: (static) global variables for class objects in extensions




Entry: Wrappers
Date: Wed Nov  4 13:06:42 CET 2009

Looking at media.c, what occurs here is:

  - CAST
  - ERROR
  - C object calls

CAST + object calls can be automated.  

For PF and SC, object wrapping might be different.  Can this be solved
at link time?  The real interface is object_to_xxx which is currently
an inline function.  If this is transformed into linkable symbols, the
binary code can be reused + plugins become feasible.

How to automate ERROR wrapper generation?  Somewhere the decision
needs to be made about how to handle errors.  Either C code returns
error values, or a setjump-based approach can be used.



Entry: Missing functionality
Date: Wed Nov  4 13:41:43 CET 2009

- current-input-port / current-output-port OK
- system OK
- string parser OK
- parameters
- let*
- macros: check if it's actually correct (evaluation order)
- readline

Can parameters be bypassed?  I'm already using implicit output ports
all over the code.  Currently just using direct (lexical) write
routines: parameters aren't really necessary.


Entry: ffmpeg delayed frames
Date: Wed Nov  4 13:59:19 CET 2009

- first frame might be zero size
- setting frame input to NULL produces delayed frames.


Entry: ffmpeg external codecs
Date: Wed Nov  4 15:05:12 CET 2009

does ffmpeg have an mp3 encoder?
no : use lame

ac3, aac, vorbis are built-in

apparently all configuration data is stored in the AVContext struct.
the ffmpeg code is quite error prone..  a bit of abstraction around it
won't hurt.

next: write encoded audio to buffer. there is no data structure for
audio - just a flat array of short int.


int avcodec_encode_audio(AVCodecContext *avctx,
                         uint8_t *buf,
                         int buf_size,
                         const short *samples);


Entry: Muxing
Date: Wed Nov  4 16:35:54 CET 2009

av_interleaved_write_frame AVPacket -> AVFormatContext

an AVFormatContext has a number of AVStream


Whether to write audio or video is based on the pts (time stamp)


        /* write interleaved audio and video frames */
        if (!video_st || (video_st && audio_st && audio_pts < video_pts)) {
            write_audio_frame(oc, audio_st);
        } else {
            write_video_frame(oc, video_st);
        }


[1] http://cekirdek.pardus.org.tr/~ismail/ffmpeg-docs/structAVPacket.html
[2] http://cekirdek.pardus.org.tr/~ismail/ffmpeg-docs/structAVStream.html
[3] http://cekirdek.pardus.org.tr/~ismail/ffmpeg-docs/structAVFormatContext.html


Entry: load
Date: Thu Nov  5 08:13:29 CET 2009

It's working: implemented in scheme, but the parser won't give eof: it
hangs.  Fixed: it actually worked, just needed to add eof-object? check.


Entry: C90
Date: Thu Nov  5 10:29:01 CET 2009

Making sure it works using C90.  How to ask GCC to check that?

gcc -ansi -pedantic

Problems:
    - task (as expected..) :

task.c:63: warning: ISO C90 forbids variable-size array reserve
task.c:63: warning: ISO C90 forbids mixed declarations and code

    - va_copy (C99)

Conclusion: apart from some annoying limitations and the task.c
abstraction there seems to be little trouble in making it compliant to
C99.  C90 is more difficult though.  Postponed until it's required.


Entry: finding home
Date: Thu Nov  5 11:13:53 CET 2009

After "make install" the binary library should be able to find its
boot code.  It's probably simplest to use pkg-config to do this.


Entry: libprim classes
Date: Thu Nov  5 13:50:26 CET 2009

Change needed: all classes need the same members, so the scripting
core can treat all objects properly.

0. free
1. write

Essentially, the class struct contains the methods, while the object
struct contains a pointer to the class struct which identifies the
object type.

The problem is that the method implementation need to be independent
of any scripting language core details.

OK works.


Entry: Parser
Date: Thu Nov  5 17:03:46 CET 2009

the leg parser gives problems.  essentially, I don't understand it.
time to switch to a different one.  maybe gut the one from tinyscheme?


Entry: GC restarts and reading
Date: Thu Nov  5 17:34:05 CET 2009

Reading is stateful: it can't trigger a restart.  This is best solved
by calling the collector before read.

Growing the heap:  restart loops need to be detected.

fixed with gc->margin check after collection.  Currently set at 100
cells.  (Heuristic: 11 is not enough for SC and triggers a restart
loop).


Entry: SRFI-1
Date: Fri Nov  6 07:51:05 CET 2009

I took the reference implementation from [1], but had to remove some
initial comments: the parser choked on realloc().

Problems keep turning up.  While it's quite elegant to express it as a
leg grammar, it's probably best to start thinking about writing one
from scratch that I actually understand and that plays nice with the
GC + supports suspension.  Weekend job.

Code parses, but some functionality is missing:

 values, receive, check-arg

[1] http://srfi.schemers.org/srfi-1/srfi-1.html


Entry: dynamic linking
Date: Fri Nov  6 11:16:27 CET 2009

All object_to_xxx and xxx_to_object calls should be accessible by the
linker.  This consists a major interface between the C lowlevel
objects/classes and the wrapped representation in the scripting layers
PF/SC/...


Entry: parsing
Date: Fri Nov  6 16:04:36 CET 2009

Using peg/leg anyway..  The problems to solve:

  - realloc problem

  - GC access since it's a side-effecting operation.


Entry: manual parsing?
Date: Sat Nov  7 07:26:48 CET 2009

Lisp syntax is LL(1) -> deterministic recursive descent.

Manually?  problems:
  - data buffering (growable arrays)
  - serialization 

First: how to serialize a parse?  Essentially, I need a linear
representation of CONS cell based s-expressions.

(1 2 3)   -> ( 1 . ( 2 . ( 3 . () ) ) )
((1 2) 3) -> ( ( 1 . ( 2 . () ) ) . ( 3 . () ) )

CONS NUMBER 1 CONS NUMBER 2 CONS NUMBER 3 NIL
CONS CONS NUMBER 1 CONS NUMBER 2 NIL CONS NUMBER 3 NIL

here CONS and NUMBER are functions that consume a fixed amount of next
items which are always functions to be executed.

Essentially, the s-expressions need to be translated to a prefix
stream.  This can be standardized.  There is only one primitive data
item: a fixed size byte array with size prefix.  The rest are
ADT constructors.  The constructors themselves could be encoded as
prefixed byte strings. 

So, in order to do this, the only thing that's needed is an encoding
of natural numbers in byte strings to represent size tags.

CONS NUMBER 1 CONS NUMBER 2 CONS NUMBER 3 NIL
-> 1C1N11C1N21C1N31X

where "C" = cons
      "N" = number
      "X" = nil

Also, to make the format completely general, the constructor sizes
need to be specified too.  So we have:

<c-name-length> <name> <c-nb-arguments> ...

So, the only problem is self-delimiting natural numbers.  Constraints:
    - small numbers are fast to decode.
    - mostly ASCII when the names themselves are ASCII


CONS NUMBER 1 CONS NUMBER 2 CONS NUMBER 3 NIL
-> 1C21N11...


Simplest way to have self-delimited byte sequences is to mark the last
digit.  This way small sizes are captured in single bytes.

Using base-16 is inefficient, but it's easiest to decode on small
targets.  This way letters can be used, with capitals denoting the
last symbol.  An advantage is that this introduces redundancy that can
be used to guess the file format.

abcd
efgh
ijkl
mnop

eHello

Better is this: use the range 0x30 "0123456789:;<=>?" for the final
symbols and 0x40 @ABC... for the prefixed.  This way the small strings
have decimal digits.

1C21N1B40x10

Even better: use the range 0x20 for prefixes: this distinguishes them
from names.

1B4fooo


Entry: Architecture summary
Date: Sat Nov  7 09:36:10 CET 2009

LEAF:  primitive C objects
EX:    C-based expression languages using GCd objects (wrapping leaf objects)
PF/SC: execution model, using EX primitive operations + data model.

caveats:

  - SC is slow: heap-allocated tree walking interpreter with
    search-based symbol binding.  The upside is that the structure is
    simple: a textbook CEKS machine.

  - EX requires distinction between two modes: 

    * PURE: construction without mutation = GC enabled -> primitives
            might restart on collection

    * LINEAR: mutation without construction = GC disabled.

